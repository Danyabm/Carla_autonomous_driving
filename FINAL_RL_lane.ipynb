{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## running code\n",
    "\n",
    "# robotuser@kuorobot02 carla_0.9.12 → python3 carla_lane_env.py \n",
    "# robotuser@kuorobot02 carla_0.9.12 → python3 carla_rl_training.py \n",
    "\n",
    "#carla_0.9.12 → python3 carla_evaluation.py  --model carla_dqn_arrow_best.pth --episodes 3\n",
    "#robotuser@kuorobot02 carla_0.9.12 → python3 carla_evaluation.py  --model carla_dqn_smooth_final.pth --episodes 5\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# carla_lane.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- robotuser@kuorobot02 carla_0.9.12 → python3 carla_evaluation.py  --model carla_dqn_arrow_best.pth --episodes 3\n",
    "\n",
    "robotuser@kuorobot02 carla_0.9.12 → python3 carla_evaluation.py  --model carla_dqn_smooth_final.pth --episodes 5\n",
    "\n",
    " robotuser@kuorobot02 carla_0.9.12 → python3 carla_rl_training.py \n",
    "\n",
    " robotuser@kuorobot02 carla_0.9.12 → python3 carla_evaluation.py  --model carla_dqn_agent_final.pth --duration 120 --speed 0.3\n",
    "\n",
    "robotuser@kuorobot02 carla_0.9.12 → python3 carla_lane_env.py  -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CARLA RL Training using DQN - WITH LANE ARROW FOLLOWING\n",
    "Focus: Smooth driving + Following lane markings (left/right arrows)\n",
    "\n",
    "Key Features:\n",
    "1. Detects lane change arrows in current lane\n",
    "2. Rewards following the arrow direction\n",
    "3. Smooth lane changes when arrows detected\n",
    "4. Enhanced state representation with lane change info\n",
    "\"\"\"\n",
    "\n",
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "# ======================= CONFIG =======================\n",
    "EPISODES = 300\n",
    "MAX_STEPS = 500\n",
    "MEMORY_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 0.995\n",
    "LEARNING_RATE = 1e-4\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# IMPROVED DISCRETE ACTIONS (smoother, more gradual)\n",
    "ACTIONS = {\n",
    "    0: (0.5, 0.0),    # Straight forward\n",
    "    1: (0.5, -0.15),  # Gentle left\n",
    "    2: (0.5, 0.15),   # Gentle right\n",
    "    3: (0.4, 0.0),    # Slow forward\n",
    "    4: (0.4, -0.15),  # Slow + gentle left\n",
    "    5: (0.4, 0.15),   # Slow + gentle right\n",
    "    6: (0.0, 0.0),    # Brake/Stop\n",
    "    7: (0.3, -0.3),   # Sharp left (for lane changes)\n",
    "    8: (0.3, 0.3),    # Sharp right (for lane changes)\n",
    "}\n",
    "\n",
    "# IMPROVED REWARDS\n",
    "REWARD_DISTANCE = 1.0          \n",
    "REWARD_SPEED = 1.5             \n",
    "REWARD_LANE_CENTER = 5.0       \n",
    "REWARD_HEADING = 3.0           \n",
    "REWARD_SMOOTHNESS = 2.0        \n",
    "REWARD_COLLISION = -200.0      \n",
    "REWARD_OFF_ROAD = -100.0       \n",
    "REWARD_LANE_INVASION = -20.0   \n",
    "REWARD_JERKY = -5.0            \n",
    "\n",
    "# NEW: Lane arrow following rewards\n",
    "REWARD_FOLLOW_ARROW = 10.0      # Strong reward for following lane arrow\n",
    "REWARD_WRONG_LANE_CHANGE = -15.0  # Penalty for changing lane wrong direction\n",
    "REWARD_SUCCESSFUL_LANE_CHANGE = 20.0  # Bonus for completing lane change correctly\n",
    "\n",
    "TARGET_SPEED = 8.0\n",
    "\n",
    "# ======================= DQN NETWORK =======================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim=18, action_dim=9):  # Increased state_dim to 18\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# ======================= REPLAY MEMORY =======================\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# ======================= IMPROVED CARLA ENVIRONMENT =======================\n",
    "class ImprovedCarlaEnv:\n",
    "    def __init__(self, town='Town04', port=2000):\n",
    "        self.client = carla.Client('localhost', port)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.load_world(town)\n",
    "        \n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = True\n",
    "        settings.fixed_delta_seconds = 0.05\n",
    "        self.world.apply_settings(settings)\n",
    "        \n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.map = self.world.get_map()\n",
    "        self.vehicle = None\n",
    "        self.sensors = []\n",
    "        \n",
    "        self.collision_hist = []\n",
    "        self.lane_invasion_hist = []\n",
    "        self.spawn_points = self.map.get_spawn_points()\n",
    "        \n",
    "        self.last_location = None\n",
    "        self.distance_traveled = 0.0\n",
    "        self.steps = 0\n",
    "        \n",
    "        # For smoothness tracking\n",
    "        self.last_steer = 0.0\n",
    "        self.last_throttle = 0.5\n",
    "        self.steer_history = deque(maxlen=5)\n",
    "        \n",
    "        # NEW: Lane change tracking\n",
    "        self.current_lane_id = None\n",
    "        self.target_lane_from_arrow = None  # \"left\", \"right\", or None\n",
    "        self.lane_change_in_progress = False\n",
    "        self.lane_change_start_time = 0\n",
    "        self.successful_lane_changes = 0\n",
    "        \n",
    "        print(\"Improved Environment initialized with Lane Arrow Following!\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self._cleanup()\n",
    "        \n",
    "        vehicle_bp = self.blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "        spawn_point = random.choice(self.spawn_points[:30])\n",
    "        \n",
    "        try:\n",
    "            self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        except:\n",
    "            spawn_point = random.choice(self.spawn_points)\n",
    "            self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        \n",
    "        # Collision sensor\n",
    "        collision_bp = self.blueprint_library.find('sensor.other.collision')\n",
    "        collision_sensor = self.world.spawn_actor(\n",
    "            collision_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        collision_sensor.listen(lambda e: self.collision_hist.append(e))\n",
    "        self.sensors.append(collision_sensor)\n",
    "        \n",
    "        # Lane invasion sensor\n",
    "        lane_bp = self.blueprint_library.find('sensor.other.lane_invasion')\n",
    "        lane_sensor = self.world.spawn_actor(\n",
    "            lane_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        lane_sensor.listen(lambda e: self.lane_invasion_hist.append(e))\n",
    "        self.sensors.append(lane_sensor)\n",
    "        \n",
    "        # Reset tracking\n",
    "        self.collision_hist.clear()\n",
    "        self.lane_invasion_hist.clear()\n",
    "        self.last_location = self.vehicle.get_location()\n",
    "        self.distance_traveled = 0.0\n",
    "        self.steps = 0\n",
    "        self.last_steer = 0.0\n",
    "        self.last_throttle = 0.5\n",
    "        self.steer_history.clear()\n",
    "        \n",
    "        # Reset lane change tracking\n",
    "        self.current_lane_id = None\n",
    "        self.target_lane_from_arrow = None\n",
    "        self.lane_change_in_progress = False\n",
    "        self.lane_change_start_time = 0\n",
    "        self.successful_lane_changes = 0\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            self.world.tick()\n",
    "        \n",
    "        # Initialize lane ID\n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        if waypoint:\n",
    "            self.current_lane_id = waypoint.lane_id\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _detect_lane_arrow(self, waypoint):\n",
    "        \"\"\"\n",
    "        Detect if current lane has left/right arrow marking\n",
    "        Returns: \"left\", \"right\", \"both\", or None\n",
    "        \"\"\"\n",
    "        if waypoint is None:\n",
    "            return None\n",
    "        \n",
    "        # Get lane change permission\n",
    "        lane_change = waypoint.lane_change\n",
    "        \n",
    "        # CARLA LaneChange enum values:\n",
    "        # - NONE: No lane change allowed\n",
    "        # - Right: Can change to right\n",
    "        # - Left: Can change to left  \n",
    "        # - Both: Can change to both sides\n",
    "        \n",
    "        if lane_change == carla.LaneChange.Left:\n",
    "            return \"left\"\n",
    "        elif lane_change == carla.LaneChange.Right:\n",
    "            return \"right\"\n",
    "        elif lane_change == carla.LaneChange.Both:\n",
    "            return \"both\"\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _check_lane_availability(self, waypoint, direction):\n",
    "        \"\"\"Check if lane change to direction is safe and valid\"\"\"\n",
    "        if waypoint is None or direction is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if direction == \"left\":\n",
    "                target_wp = waypoint.get_left_lane()\n",
    "            elif direction == \"right\":\n",
    "                target_wp = waypoint.get_right_lane()\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "            if target_wp is None:\n",
    "                return False\n",
    "            \n",
    "            # Must be driving lane\n",
    "            if target_wp.lane_type != carla.LaneType.Driving:\n",
    "                return False\n",
    "            \n",
    "            # Must be same direction\n",
    "            if (waypoint.lane_id > 0) != (target_wp.lane_id > 0):\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Enhanced state representation WITH lane arrow info\"\"\"\n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        waypoint = self.map.get_waypoint(\n",
    "            transform.location, \n",
    "            project_to_road=True, \n",
    "            lane_type=carla.LaneType.Driving\n",
    "        )\n",
    "        \n",
    "        if waypoint is None:\n",
    "            return np.zeros(18, dtype=np.float32)\n",
    "        \n",
    "        # Lane offset (cross-track error)\n",
    "        lane_center = waypoint.transform.location\n",
    "        offset_vec = transform.location - lane_center\n",
    "        right_vec = transform.get_right_vector()\n",
    "        lane_offset = offset_vec.x * right_vec.x + offset_vec.y * right_vec.y\n",
    "        \n",
    "        # Heading error\n",
    "        lane_yaw = waypoint.transform.rotation.yaw\n",
    "        vehicle_yaw = transform.rotation.yaw\n",
    "        heading_error = (vehicle_yaw - lane_yaw + 180) % 360 - 180\n",
    "        heading_error_rad = np.radians(heading_error)\n",
    "        \n",
    "        # Look-ahead waypoint\n",
    "        next_waypoints = waypoint.next(5.0)\n",
    "        if next_waypoints:\n",
    "            next_wp = next_waypoints[0]\n",
    "            wp_vec = next_wp.transform.location - transform.location\n",
    "            forward_vec = transform.get_forward_vector()\n",
    "            angle_to_wp = np.arctan2(\n",
    "                wp_vec.y * forward_vec.x - wp_vec.x * forward_vec.y,\n",
    "                wp_vec.x * forward_vec.x + wp_vec.y * forward_vec.y\n",
    "            )\n",
    "        else:\n",
    "            angle_to_wp = 0.0\n",
    "        \n",
    "        # Road curvature\n",
    "        curvature = 0.0\n",
    "        if next_waypoints:\n",
    "            next_wp = next_waypoints[0]\n",
    "            curvature_yaw = next_wp.transform.rotation.yaw - lane_yaw\n",
    "            curvature = np.sin(np.radians(curvature_yaw))\n",
    "        \n",
    "        # Traffic light state\n",
    "        tl_state = 0.0\n",
    "        if self.vehicle.is_at_traffic_light():\n",
    "            tl = self.vehicle.get_traffic_light()\n",
    "            if tl:\n",
    "                state = tl.get_state()\n",
    "                if state == carla.TrafficLightState.Green:\n",
    "                    tl_state = 0.33\n",
    "                elif state == carla.TrafficLightState.Yellow:\n",
    "                    tl_state = 0.66\n",
    "                elif state == carla.TrafficLightState.Red:\n",
    "                    tl_state = 1.0\n",
    "        \n",
    "        # Steering smoothness\n",
    "        steer_variance = np.std(list(self.steer_history)) if len(self.steer_history) > 2 else 0.0\n",
    "        \n",
    "        # NEW: Lane arrow detection\n",
    "        lane_arrow = self._detect_lane_arrow(waypoint)\n",
    "        \n",
    "        # Encode lane arrow as three binary flags\n",
    "        arrow_left = 1.0 if lane_arrow in [\"left\", \"both\"] else 0.0\n",
    "        arrow_right = 1.0 if lane_arrow in [\"right\", \"both\"] else 0.0\n",
    "        arrow_straight = 1.0 if lane_arrow is None else 0.0\n",
    "        \n",
    "        # Check if lanes are actually available\n",
    "        can_change_left = 1.0 if self._check_lane_availability(waypoint, \"left\") else 0.0\n",
    "        can_change_right = 1.0 if self._check_lane_availability(waypoint, \"right\") else 0.0\n",
    "        \n",
    "        # Update target from arrow (only if not already in lane change)\n",
    "        if not self.lane_change_in_progress:\n",
    "            if arrow_left and can_change_left:\n",
    "                self.target_lane_from_arrow = \"left\"\n",
    "            elif arrow_right and can_change_right:\n",
    "                self.target_lane_from_arrow = \"right\"\n",
    "            else:\n",
    "                self.target_lane_from_arrow = None\n",
    "        \n",
    "        state = np.array([\n",
    "            speed / 30.0,                          # 0: Normalized speed\n",
    "            lane_offset / 3.5,                     # 1: Lane offset\n",
    "            np.sin(heading_error_rad),             # 2: Heading error (sin)\n",
    "            np.cos(heading_error_rad),             # 3: Heading error (cos)\n",
    "            np.sin(angle_to_wp),                   # 4: Angle to waypoint (sin)\n",
    "            np.cos(angle_to_wp),                   # 5: Angle to waypoint (cos)\n",
    "            curvature,                             # 6: Road curvature\n",
    "            velocity.x / 30.0,                     # 7: Velocity X\n",
    "            velocity.y / 30.0,                     # 8: Velocity Y\n",
    "            float(waypoint.is_junction),           # 9: Junction flag\n",
    "            tl_state,                              # 10: Traffic light state\n",
    "            self.last_steer,                       # 11: Previous steering\n",
    "            steer_variance,                        # 12: Steering smoothness\n",
    "            arrow_left,                            # 13: Lane arrow left\n",
    "            arrow_right,                           # 14: Lane arrow right\n",
    "            arrow_straight,                        # 15: No arrow (straight)\n",
    "            can_change_left,                       # 16: Left lane available\n",
    "            can_change_right,                      # 17: Right lane available\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action_idx):\n",
    "        \"\"\"Execute action with smoothing\"\"\"\n",
    "        throttle_raw, steer_raw = ACTIONS[action_idx]\n",
    "        \n",
    "        # Apply smoothing\n",
    "        alpha = 0.3\n",
    "        throttle = alpha * throttle_raw + (1 - alpha) * self.last_throttle\n",
    "        steer = alpha * steer_raw + (1 - alpha) * self.last_steer\n",
    "        \n",
    "        # Limit steering rate of change\n",
    "        max_steer_change = 0.1\n",
    "        steer_change = np.clip(steer - self.last_steer, -max_steer_change, max_steer_change)\n",
    "        steer = self.last_steer + steer_change\n",
    "        \n",
    "        # Store for next iteration\n",
    "        self.last_steer = steer\n",
    "        self.last_throttle = throttle\n",
    "        self.steer_history.append(steer)\n",
    "        \n",
    "        # Apply control\n",
    "        control = carla.VehicleControl(\n",
    "            throttle=float(throttle),\n",
    "            steer=float(steer),\n",
    "            brake=0.0 if throttle > 0 else 0.5\n",
    "        )\n",
    "        \n",
    "        self.vehicle.apply_control(control)\n",
    "        self.world.tick()\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Track lane changes\n",
    "        prev_invasions = len(self.lane_invasion_hist)\n",
    "        prev_lane_id = self.current_lane_id\n",
    "        \n",
    "        # Update distance\n",
    "        current_location = self.vehicle.get_location()\n",
    "        distance_step = 0.0\n",
    "        if self.last_location:\n",
    "            distance_step = current_location.distance(self.last_location)\n",
    "            self.distance_traveled += distance_step\n",
    "        self.last_location = current_location\n",
    "        \n",
    "        # Update current lane\n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        if waypoint:\n",
    "            self.current_lane_id = waypoint.lane_id\n",
    "        \n",
    "        # Detect lane change completion\n",
    "        lane_changed = False\n",
    "        lane_change_direction = None\n",
    "        if prev_lane_id is not None and self.current_lane_id != prev_lane_id:\n",
    "            lane_changed = True\n",
    "            # Determine direction (left = positive lane_id change, right = negative)\n",
    "            if self.current_lane_id > prev_lane_id:\n",
    "                lane_change_direction = \"left\"\n",
    "            else:\n",
    "                lane_change_direction = \"right\"\n",
    "            \n",
    "            # Check if it matches the arrow\n",
    "            if lane_change_direction == self.target_lane_from_arrow:\n",
    "                self.successful_lane_changes += 1\n",
    "            \n",
    "            # Reset lane change tracking\n",
    "            self.lane_change_in_progress = False\n",
    "            self.target_lane_from_arrow = None\n",
    "        \n",
    "        # Detect lane change initiation (significant lateral velocity)\n",
    "        if not self.lane_change_in_progress and abs(steer_raw) > 0.2:\n",
    "            self.lane_change_in_progress = True\n",
    "            self.lane_change_start_time = self.steps\n",
    "        \n",
    "        # Get new state\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward(\n",
    "            distance_step, \n",
    "            prev_invasions, \n",
    "            steer_change,\n",
    "            lane_changed,\n",
    "            lane_change_direction\n",
    "        )\n",
    "        \n",
    "        # Check done\n",
    "        done = self._is_done()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def _calculate_reward(self, distance_step, prev_invasions, steer_change, \n",
    "                         lane_changed, lane_change_direction):\n",
    "        \"\"\"Improved reward with lane arrow following\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        # 1. Forward progress\n",
    "        reward += REWARD_DISTANCE * distance_step\n",
    "        \n",
    "        # 2. Speed reward\n",
    "        speed_error = abs(speed - TARGET_SPEED)\n",
    "        speed_reward = REWARD_SPEED * np.exp(-speed_error / 5.0)\n",
    "        reward += speed_reward\n",
    "        \n",
    "        # 3. Lane keeping reward\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        \n",
    "        if waypoint:\n",
    "            lane_center = waypoint.transform.location\n",
    "            offset_vec = transform.location - lane_center\n",
    "            right_vec = transform.get_right_vector()\n",
    "            lane_offset = abs(offset_vec.x * right_vec.x + offset_vec.y * right_vec.y)\n",
    "            \n",
    "            lane_reward = REWARD_LANE_CENTER * np.exp(-lane_offset * 3.0)\n",
    "            reward += lane_reward\n",
    "            \n",
    "            # Heading alignment\n",
    "            lane_yaw = waypoint.transform.rotation.yaw\n",
    "            vehicle_yaw = transform.rotation.yaw\n",
    "            heading_error = abs((vehicle_yaw - lane_yaw + 180) % 360 - 180)\n",
    "            heading_reward = REWARD_HEADING * np.exp(-heading_error / 30.0)\n",
    "            reward += heading_reward\n",
    "        else:\n",
    "            reward += REWARD_OFF_ROAD\n",
    "        \n",
    "        # 4. Smoothness reward\n",
    "        steer_jerk = abs(steer_change)\n",
    "        if steer_jerk > 0.05:\n",
    "            reward += REWARD_JERKY * steer_jerk\n",
    "        else:\n",
    "            reward += REWARD_SMOOTHNESS * (0.05 - steer_jerk)\n",
    "        \n",
    "        # 5. Lane invasion penalty\n",
    "        if len(self.lane_invasion_hist) > prev_invasions:\n",
    "            reward += REWARD_LANE_INVASION\n",
    "        \n",
    "        # 6. Collision penalty\n",
    "        if len(self.collision_hist) > 0:\n",
    "            reward += REWARD_COLLISION\n",
    "        \n",
    "        # 7. Bonus for good driving\n",
    "        if waypoint and lane_offset < 0.5 and speed > 5.0:\n",
    "            reward += 1.0\n",
    "        \n",
    "        # NEW: 8. Lane arrow following rewards\n",
    "        if lane_changed:\n",
    "            if lane_change_direction == self.target_lane_from_arrow:\n",
    "                # Correct lane change following arrow!\n",
    "                reward += REWARD_SUCCESSFUL_LANE_CHANGE\n",
    "                print(f\"  ✓ Successful {lane_change_direction} lane change following arrow!\")\n",
    "            else:\n",
    "                # Wrong direction lane change\n",
    "                reward += REWARD_WRONG_LANE_CHANGE\n",
    "        \n",
    "        # NEW: 9. Continuous reward for moving toward arrow direction\n",
    "        if self.target_lane_from_arrow is not None and waypoint:\n",
    "            # Small reward for steering in correct direction when arrow present\n",
    "            if self.target_lane_from_arrow == \"left\" and self.last_steer < -0.05:\n",
    "                reward += REWARD_FOLLOW_ARROW * 0.1  # Small continuous reward\n",
    "            elif self.target_lane_from_arrow == \"right\" and self.last_steer > 0.05:\n",
    "                reward += REWARD_FOLLOW_ARROW * 0.1\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _is_done(self):\n",
    "        \"\"\"Check termination\"\"\"\n",
    "        if len(self.collision_hist) > 0:\n",
    "            return True\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        \n",
    "        if waypoint is None:\n",
    "            return True\n",
    "        \n",
    "        if transform.location.distance(waypoint.transform.location) > 4.0:\n",
    "            return True\n",
    "        \n",
    "        if self.steps >= MAX_STEPS:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        \"\"\"Cleanup\"\"\"\n",
    "        if self.vehicle:\n",
    "            self.vehicle.destroy()\n",
    "        for sensor in self.sensors:\n",
    "            if sensor.is_alive:\n",
    "                sensor.destroy()\n",
    "        self.sensors.clear()\n",
    "        self.vehicle = None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close environment\"\"\"\n",
    "        self._cleanup()\n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = False\n",
    "        self.world.apply_settings(settings)\n",
    "\n",
    "# ======================= DQN AGENT =======================\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim=18, action_dim=9):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = ReplayMemory(MEMORY_SIZE)\n",
    "        \n",
    "        self.epsilon = EPSILON_START\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update network\"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0.0\n",
    "        \n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = list(zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.FloatTensor(np.array(batch[0])).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch[1]).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch[2]).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch[3])).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch[4]).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_state_batch).argmax(1, keepdim=True)\n",
    "            next_q = self.target_net(next_state_batch).gather(1, next_actions).squeeze()\n",
    "            target_q = reward_batch + (1 - done_batch) * GAMMA * next_q\n",
    "        \n",
    "        loss = nn.SmoothL1Loss()(current_q.squeeze(), target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration\"\"\"\n",
    "        self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)\n",
    "\n",
    "# ======================= TRAINING =======================\n",
    "def train():\n",
    "    print(\"=\"*70)\n",
    "    print(\"CARLA DQN TRAINING - SMOOTH DRIVING + LANE ARROW FOLLOWING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    env = ImprovedCarlaEnv()\n",
    "    agent = DQNAgent(state_dim=18, action_dim=9)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_distances = []\n",
    "    episode_lengths = []\n",
    "    episode_lane_invasions = []\n",
    "    episode_lane_changes = []\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"Focus: Smooth control + Lane keeping + Following lane arrows\\n\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_avg_distance = 0.0\n",
    "    \n",
    "    for episode in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            agent.memory.push(state, action, reward, next_state, float(done))\n",
    "            loss = agent.update()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        if (episode + 1) % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_distances.append(env.distance_traveled)\n",
    "        episode_lengths.append(env.steps)\n",
    "        episode_lane_invasions.append(len(env.lane_invasion_hist))\n",
    "        episode_lane_changes.append(env.successful_lane_changes)\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_distance = np.mean(episode_distances[-10:])\n",
    "            avg_length = np.mean(episode_lengths[-10:])\n",
    "            avg_invasions = np.mean(episode_lane_invasions[-10:])\n",
    "            avg_changes = np.mean(episode_lane_changes[-10:])\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            \n",
    "            print(f\"Ep {episode+1:3d} | \"\n",
    "                  f\"R: {episode_reward:7.1f} | \"\n",
    "                  f\"D: {env.distance_traveled:6.1f}m | \"\n",
    "                  f\"L: {env.steps:3d} | \"\n",
    "                  f\"LI: {len(env.lane_invasion_hist):2d} | \"\n",
    "                  f\"LC: {env.successful_lane_changes} | \"\n",
    "                  f\"ε: {agent.epsilon:.3f} | \"\n",
    "                  f\"Avg10: R={avg_reward:6.1f}, D={avg_distance:5.1f}m, LC={avg_changes:.1f} | \"\n",
    "                  f\"T: {elapsed:.1f}min\")\n",
    "            \n",
    "            if avg_distance > best_avg_distance:\n",
    "                best_avg_distance = avg_distance\n",
    "                torch.save(agent.policy_net.state_dict(), 'carla_dqn_arrow_best.pth')\n",
    "                print(f\"  ✓ New best model saved! Avg distance: {avg_distance:.1f}m\")\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            torch.save(agent.policy_net.state_dict(), f'dqn_arrow_ep{episode+1}.pth')\n",
    "            print(f\"  ✓ Checkpoint saved\")\n",
    "    \n",
    "    torch.save(agent.policy_net.state_dict(), 'carla_dqn_arrow_final.pth')\n",
    "    print(f\"\\n✓ Training completed in {(time.time() - start_time) / 60:.1f} minutes\")\n",
    "    print(\"✓ Final model saved: carla_dqn_arrow_final.pth\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Best avg distance (10 ep): {best_avg_distance:.1f}m\")\n",
    "    print(f\"Final avg reward (last 20): {np.mean(episode_rewards[-20:]):.1f}\")\n",
    "    print(f\"Final avg distance (last 20): {np.mean(episode_distances[-20:]):.1f}m\")\n",
    "    print(f\"Final avg lane invasions (last 20): {np.mean(episode_lane_invasions[-20:]):.1f}\")\n",
    "    print(f\"Final avg successful lane changes (last 20): {np.mean(episode_lane_changes[-20:]):.1f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# carla_rl_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dqn_arrow_best.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CARLA RL Training using DQN - WITH LANE ARROW FOLLOWING\n",
    "Focus: Smooth driving + Following lane markings (left/right arrows)\n",
    "\n",
    "Key Features:\n",
    "1. Detects lane change arrows in current lane\n",
    "2. Rewards following the arrow direction\n",
    "3. Smooth lane changes when arrows detected\n",
    "4. Enhanced state representation with lane change info\n",
    "\"\"\"\n",
    "\n",
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "# ======================= CONFIG =======================\n",
    "EPISODES = 300\n",
    "MAX_STEPS = 500\n",
    "MEMORY_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 0.995\n",
    "LEARNING_RATE = 1e-4\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# IMPROVED DISCRETE ACTIONS (smoother, more gradual)\n",
    "ACTIONS = {\n",
    "    0: (0.5, 0.0),    # Straight forward\n",
    "    1: (0.5, -0.15),  # Gentle left\n",
    "    2: (0.5, 0.15),   # Gentle right\n",
    "    3: (0.4, 0.0),    # Slow forward\n",
    "    4: (0.4, -0.15),  # Slow + gentle left\n",
    "    5: (0.4, 0.15),   # Slow + gentle right\n",
    "    6: (0.0, 0.0),    # Brake/Stop\n",
    "    7: (0.3, -0.3),   # Sharp left (for lane changes)\n",
    "    8: (0.3, 0.3),    # Sharp right (for lane changes)\n",
    "}\n",
    "\n",
    "# IMPROVED REWARDS\n",
    "REWARD_DISTANCE = 1.0          \n",
    "REWARD_SPEED = 1.5             \n",
    "REWARD_LANE_CENTER = 5.0       \n",
    "REWARD_HEADING = 3.0           \n",
    "REWARD_SMOOTHNESS = 2.0        \n",
    "REWARD_COLLISION = -200.0      \n",
    "REWARD_OFF_ROAD = -100.0       \n",
    "REWARD_LANE_INVASION = -20.0   \n",
    "REWARD_JERKY = -5.0            \n",
    "\n",
    "# NEW: Lane arrow following rewards\n",
    "REWARD_FOLLOW_ARROW = 10.0      # Strong reward for following lane arrow\n",
    "REWARD_WRONG_LANE_CHANGE = -15.0  # Penalty for changing lane wrong direction\n",
    "REWARD_SUCCESSFUL_LANE_CHANGE = 20.0  # Bonus for completing lane change correctly\n",
    "\n",
    "TARGET_SPEED = 8.0\n",
    "\n",
    "# ======================= DQN NETWORK =======================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim=18, action_dim=9):  # Increased state_dim to 18\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# ======================= REPLAY MEMORY =======================\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# ======================= IMPROVED CARLA ENVIRONMENT =======================\n",
    "class ImprovedCarlaEnv:\n",
    "    def __init__(self, town='Town04', port=2000):\n",
    "        self.client = carla.Client('localhost', port)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.load_world(town)\n",
    "        \n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = True\n",
    "        settings.fixed_delta_seconds = 0.05\n",
    "        self.world.apply_settings(settings)\n",
    "        \n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.map = self.world.get_map()\n",
    "        self.vehicle = None\n",
    "        self.sensors = []\n",
    "        \n",
    "        self.collision_hist = []\n",
    "        self.lane_invasion_hist = []\n",
    "        self.spawn_points = self.map.get_spawn_points()\n",
    "        \n",
    "        self.last_location = None\n",
    "        self.distance_traveled = 0.0\n",
    "        self.steps = 0\n",
    "        \n",
    "        # For smoothness tracking\n",
    "        self.last_steer = 0.0\n",
    "        self.last_throttle = 0.5\n",
    "        self.steer_history = deque(maxlen=5)\n",
    "        \n",
    "        # NEW: Lane change tracking\n",
    "        self.current_lane_id = None\n",
    "        self.target_lane_from_arrow = None  # \"left\", \"right\", or None\n",
    "        self.lane_change_in_progress = False\n",
    "        self.lane_change_start_time = 0\n",
    "        self.successful_lane_changes = 0\n",
    "        \n",
    "        print(\"Improved Environment initialized with Lane Arrow Following!\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self._cleanup()\n",
    "        \n",
    "        vehicle_bp = self.blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "        spawn_point = random.choice(self.spawn_points[:30])\n",
    "        \n",
    "        try:\n",
    "            self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        except:\n",
    "            spawn_point = random.choice(self.spawn_points)\n",
    "            self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        \n",
    "        # Collision sensor\n",
    "        collision_bp = self.blueprint_library.find('sensor.other.collision')\n",
    "        collision_sensor = self.world.spawn_actor(\n",
    "            collision_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        collision_sensor.listen(lambda e: self.collision_hist.append(e))\n",
    "        self.sensors.append(collision_sensor)\n",
    "        \n",
    "        # Lane invasion sensor\n",
    "        lane_bp = self.blueprint_library.find('sensor.other.lane_invasion')\n",
    "        lane_sensor = self.world.spawn_actor(\n",
    "            lane_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        lane_sensor.listen(lambda e: self.lane_invasion_hist.append(e))\n",
    "        self.sensors.append(lane_sensor)\n",
    "        \n",
    "        # Reset tracking\n",
    "        self.collision_hist.clear()\n",
    "        self.lane_invasion_hist.clear()\n",
    "        self.last_location = self.vehicle.get_location()\n",
    "        self.distance_traveled = 0.0\n",
    "        self.steps = 0\n",
    "        self.last_steer = 0.0\n",
    "        self.last_throttle = 0.5\n",
    "        self.steer_history.clear()\n",
    "        \n",
    "        # Reset lane change tracking\n",
    "        self.current_lane_id = None\n",
    "        self.target_lane_from_arrow = None\n",
    "        self.lane_change_in_progress = False\n",
    "        self.lane_change_start_time = 0\n",
    "        self.successful_lane_changes = 0\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            self.world.tick()\n",
    "        \n",
    "        # Initialize lane ID\n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        if waypoint:\n",
    "            self.current_lane_id = waypoint.lane_id\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _detect_lane_arrow(self, waypoint):\n",
    "        \"\"\"\n",
    "        Detect if current lane has left/right arrow marking\n",
    "        Returns: \"left\", \"right\", \"both\", or None\n",
    "        \"\"\"\n",
    "        if waypoint is None:\n",
    "            return None\n",
    "        \n",
    "        # Get lane change permission\n",
    "        lane_change = waypoint.lane_change\n",
    "        \n",
    "        # CARLA LaneChange enum values:\n",
    "        # - NONE: No lane change allowed\n",
    "        # - Right: Can change to right\n",
    "        # - Left: Can change to left  \n",
    "        # - Both: Can change to both sides\n",
    "        \n",
    "        if lane_change == carla.LaneChange.Left:\n",
    "            return \"left\"\n",
    "        elif lane_change == carla.LaneChange.Right:\n",
    "            return \"right\"\n",
    "        elif lane_change == carla.LaneChange.Both:\n",
    "            return \"both\"\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _check_lane_availability(self, waypoint, direction):\n",
    "        \"\"\"Check if lane change to direction is safe and valid\"\"\"\n",
    "        if waypoint is None or direction is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if direction == \"left\":\n",
    "                target_wp = waypoint.get_left_lane()\n",
    "            elif direction == \"right\":\n",
    "                target_wp = waypoint.get_right_lane()\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "            if target_wp is None:\n",
    "                return False\n",
    "            \n",
    "            # Must be driving lane\n",
    "            if target_wp.lane_type != carla.LaneType.Driving:\n",
    "                return False\n",
    "            \n",
    "            # Must be same direction\n",
    "            if (waypoint.lane_id > 0) != (target_wp.lane_id > 0):\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Enhanced state representation WITH lane arrow info\"\"\"\n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        waypoint = self.map.get_waypoint(\n",
    "            transform.location, \n",
    "            project_to_road=True, \n",
    "            lane_type=carla.LaneType.Driving\n",
    "        )\n",
    "        \n",
    "        if waypoint is None:\n",
    "            return np.zeros(18, dtype=np.float32)\n",
    "        \n",
    "        # Lane offset (cross-track error)\n",
    "        lane_center = waypoint.transform.location\n",
    "        offset_vec = transform.location - lane_center\n",
    "        right_vec = transform.get_right_vector()\n",
    "        lane_offset = offset_vec.x * right_vec.x + offset_vec.y * right_vec.y\n",
    "        \n",
    "        # Heading error\n",
    "        lane_yaw = waypoint.transform.rotation.yaw\n",
    "        vehicle_yaw = transform.rotation.yaw\n",
    "        heading_error = (vehicle_yaw - lane_yaw + 180) % 360 - 180\n",
    "        heading_error_rad = np.radians(heading_error)\n",
    "        \n",
    "        # Look-ahead waypoint\n",
    "        next_waypoints = waypoint.next(5.0)\n",
    "        if next_waypoints:\n",
    "            next_wp = next_waypoints[0]\n",
    "            wp_vec = next_wp.transform.location - transform.location\n",
    "            forward_vec = transform.get_forward_vector()\n",
    "            angle_to_wp = np.arctan2(\n",
    "                wp_vec.y * forward_vec.x - wp_vec.x * forward_vec.y,\n",
    "                wp_vec.x * forward_vec.x + wp_vec.y * forward_vec.y\n",
    "            )\n",
    "        else:\n",
    "            angle_to_wp = 0.0\n",
    "        \n",
    "        # Road curvature\n",
    "        curvature = 0.0\n",
    "        if next_waypoints:\n",
    "            next_wp = next_waypoints[0]\n",
    "            curvature_yaw = next_wp.transform.rotation.yaw - lane_yaw\n",
    "            curvature = np.sin(np.radians(curvature_yaw))\n",
    "        \n",
    "        # Traffic light state\n",
    "        tl_state = 0.0\n",
    "        if self.vehicle.is_at_traffic_light():\n",
    "            tl = self.vehicle.get_traffic_light()\n",
    "            if tl:\n",
    "                state = tl.get_state()\n",
    "                if state == carla.TrafficLightState.Green:\n",
    "                    tl_state = 0.33\n",
    "                elif state == carla.TrafficLightState.Yellow:\n",
    "                    tl_state = 0.66\n",
    "                elif state == carla.TrafficLightState.Red:\n",
    "                    tl_state = 1.0\n",
    "        \n",
    "        # Steering smoothness\n",
    "        steer_variance = np.std(list(self.steer_history)) if len(self.steer_history) > 2 else 0.0\n",
    "        \n",
    "        # NEW: Lane arrow detection\n",
    "        lane_arrow = self._detect_lane_arrow(waypoint)\n",
    "        \n",
    "        # Encode lane arrow as three binary flags\n",
    "        arrow_left = 1.0 if lane_arrow in [\"left\", \"both\"] else 0.0\n",
    "        arrow_right = 1.0 if lane_arrow in [\"right\", \"both\"] else 0.0\n",
    "        arrow_straight = 1.0 if lane_arrow is None else 0.0\n",
    "        \n",
    "        # Check if lanes are actually available\n",
    "        can_change_left = 1.0 if self._check_lane_availability(waypoint, \"left\") else 0.0\n",
    "        can_change_right = 1.0 if self._check_lane_availability(waypoint, \"right\") else 0.0\n",
    "        \n",
    "        # Update target from arrow (only if not already in lane change)\n",
    "        if not self.lane_change_in_progress:\n",
    "            if arrow_left and can_change_left:\n",
    "                self.target_lane_from_arrow = \"left\"\n",
    "            elif arrow_right and can_change_right:\n",
    "                self.target_lane_from_arrow = \"right\"\n",
    "            else:\n",
    "                self.target_lane_from_arrow = None\n",
    "        \n",
    "        state = np.array([\n",
    "            speed / 30.0,                          # 0: Normalized speed\n",
    "            lane_offset / 3.5,                     # 1: Lane offset\n",
    "            np.sin(heading_error_rad),             # 2: Heading error (sin)\n",
    "            np.cos(heading_error_rad),             # 3: Heading error (cos)\n",
    "            np.sin(angle_to_wp),                   # 4: Angle to waypoint (sin)\n",
    "            np.cos(angle_to_wp),                   # 5: Angle to waypoint (cos)\n",
    "            curvature,                             # 6: Road curvature\n",
    "            velocity.x / 30.0,                     # 7: Velocity X\n",
    "            velocity.y / 30.0,                     # 8: Velocity Y\n",
    "            float(waypoint.is_junction),           # 9: Junction flag\n",
    "            tl_state,                              # 10: Traffic light state\n",
    "            self.last_steer,                       # 11: Previous steering\n",
    "            steer_variance,                        # 12: Steering smoothness\n",
    "            arrow_left,                            # 13: Lane arrow left\n",
    "            arrow_right,                           # 14: Lane arrow right\n",
    "            arrow_straight,                        # 15: No arrow (straight)\n",
    "            can_change_left,                       # 16: Left lane available\n",
    "            can_change_right,                      # 17: Right lane available\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action_idx):\n",
    "        \"\"\"Execute action with smoothing\"\"\"\n",
    "        throttle_raw, steer_raw = ACTIONS[action_idx]\n",
    "        \n",
    "        # Apply smoothing\n",
    "        alpha = 0.3\n",
    "        throttle = alpha * throttle_raw + (1 - alpha) * self.last_throttle\n",
    "        steer = alpha * steer_raw + (1 - alpha) * self.last_steer\n",
    "        \n",
    "        # Limit steering rate of change\n",
    "        max_steer_change = 0.1\n",
    "        steer_change = np.clip(steer - self.last_steer, -max_steer_change, max_steer_change)\n",
    "        steer = self.last_steer + steer_change\n",
    "        \n",
    "        # Store for next iteration\n",
    "        self.last_steer = steer\n",
    "        self.last_throttle = throttle\n",
    "        self.steer_history.append(steer)\n",
    "        \n",
    "        # Apply control\n",
    "        control = carla.VehicleControl(\n",
    "            throttle=float(throttle),\n",
    "            steer=float(steer),\n",
    "            brake=0.0 if throttle > 0 else 0.5\n",
    "        )\n",
    "        \n",
    "        self.vehicle.apply_control(control)\n",
    "        self.world.tick()\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Track lane changes\n",
    "        prev_invasions = len(self.lane_invasion_hist)\n",
    "        prev_lane_id = self.current_lane_id\n",
    "        \n",
    "        # Update distance\n",
    "        current_location = self.vehicle.get_location()\n",
    "        distance_step = 0.0\n",
    "        if self.last_location:\n",
    "            distance_step = current_location.distance(self.last_location)\n",
    "            self.distance_traveled += distance_step\n",
    "        self.last_location = current_location\n",
    "        \n",
    "        # Update current lane\n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        if waypoint:\n",
    "            self.current_lane_id = waypoint.lane_id\n",
    "        \n",
    "        # Detect lane change completion\n",
    "        lane_changed = False\n",
    "        lane_change_direction = None\n",
    "        if prev_lane_id is not None and self.current_lane_id != prev_lane_id:\n",
    "            lane_changed = True\n",
    "            # Determine direction (left = positive lane_id change, right = negative)\n",
    "            if self.current_lane_id > prev_lane_id:\n",
    "                lane_change_direction = \"left\"\n",
    "            else:\n",
    "                lane_change_direction = \"right\"\n",
    "            \n",
    "            # Check if it matches the arrow\n",
    "            if lane_change_direction == self.target_lane_from_arrow:\n",
    "                self.successful_lane_changes += 1\n",
    "            \n",
    "            # Reset lane change tracking\n",
    "            self.lane_change_in_progress = False\n",
    "            self.target_lane_from_arrow = None\n",
    "        \n",
    "        # Detect lane change initiation (significant lateral velocity)\n",
    "        if not self.lane_change_in_progress and abs(steer_raw) > 0.2:\n",
    "            self.lane_change_in_progress = True\n",
    "            self.lane_change_start_time = self.steps\n",
    "        \n",
    "        # Get new state\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward(\n",
    "            distance_step, \n",
    "            prev_invasions, \n",
    "            steer_change,\n",
    "            lane_changed,\n",
    "            lane_change_direction\n",
    "        )\n",
    "        \n",
    "        # Check done\n",
    "        done = self._is_done()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def _calculate_reward(self, distance_step, prev_invasions, steer_change, \n",
    "                         lane_changed, lane_change_direction):\n",
    "        \"\"\"Improved reward with lane arrow following\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        # 1. Forward progress\n",
    "        reward += REWARD_DISTANCE * distance_step\n",
    "        \n",
    "        # 2. Speed reward\n",
    "        speed_error = abs(speed - TARGET_SPEED)\n",
    "        speed_reward = REWARD_SPEED * np.exp(-speed_error / 5.0)\n",
    "        reward += speed_reward\n",
    "        \n",
    "        # 3. Lane keeping reward\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        \n",
    "        if waypoint:\n",
    "            lane_center = waypoint.transform.location\n",
    "            offset_vec = transform.location - lane_center\n",
    "            right_vec = transform.get_right_vector()\n",
    "            lane_offset = abs(offset_vec.x * right_vec.x + offset_vec.y * right_vec.y)\n",
    "            \n",
    "            lane_reward = REWARD_LANE_CENTER * np.exp(-lane_offset * 3.0)\n",
    "            reward += lane_reward\n",
    "            \n",
    "            # Heading alignment\n",
    "            lane_yaw = waypoint.transform.rotation.yaw\n",
    "            vehicle_yaw = transform.rotation.yaw\n",
    "            heading_error = abs((vehicle_yaw - lane_yaw + 180) % 360 - 180)\n",
    "            heading_reward = REWARD_HEADING * np.exp(-heading_error / 30.0)\n",
    "            reward += heading_reward\n",
    "        else:\n",
    "            reward += REWARD_OFF_ROAD\n",
    "        \n",
    "        # 4. Smoothness reward\n",
    "        steer_jerk = abs(steer_change)\n",
    "        if steer_jerk > 0.05:\n",
    "            reward += REWARD_JERKY * steer_jerk\n",
    "        else:\n",
    "            reward += REWARD_SMOOTHNESS * (0.05 - steer_jerk)\n",
    "        \n",
    "        # 5. Lane invasion penalty\n",
    "        if len(self.lane_invasion_hist) > prev_invasions:\n",
    "            reward += REWARD_LANE_INVASION\n",
    "        \n",
    "        # 6. Collision penalty\n",
    "        if len(self.collision_hist) > 0:\n",
    "            reward += REWARD_COLLISION\n",
    "        \n",
    "        # 7. Bonus for good driving\n",
    "        if waypoint and lane_offset < 0.5 and speed > 5.0:\n",
    "            reward += 1.0\n",
    "        \n",
    "        # NEW: 8. Lane arrow following rewards\n",
    "        if lane_changed:\n",
    "            if lane_change_direction == self.target_lane_from_arrow:\n",
    "                # Correct lane change following arrow!\n",
    "                reward += REWARD_SUCCESSFUL_LANE_CHANGE\n",
    "                print(f\"  ✓ Successful {lane_change_direction} lane change following arrow!\")\n",
    "            else:\n",
    "                # Wrong direction lane change\n",
    "                reward += REWARD_WRONG_LANE_CHANGE\n",
    "        \n",
    "        # NEW: 9. Continuous reward for moving toward arrow direction\n",
    "        if self.target_lane_from_arrow is not None and waypoint:\n",
    "            # Small reward for steering in correct direction when arrow present\n",
    "            if self.target_lane_from_arrow == \"left\" and self.last_steer < -0.05:\n",
    "                reward += REWARD_FOLLOW_ARROW * 0.1  # Small continuous reward\n",
    "            elif self.target_lane_from_arrow == \"right\" and self.last_steer > 0.05:\n",
    "                reward += REWARD_FOLLOW_ARROW * 0.1\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _is_done(self):\n",
    "        \"\"\"Check termination\"\"\"\n",
    "        if len(self.collision_hist) > 0:\n",
    "            return True\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        \n",
    "        if waypoint is None:\n",
    "            return True\n",
    "        \n",
    "        if transform.location.distance(waypoint.transform.location) > 4.0:\n",
    "            return True\n",
    "        \n",
    "        if self.steps >= MAX_STEPS:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        \"\"\"Cleanup\"\"\"\n",
    "        if self.vehicle:\n",
    "            self.vehicle.destroy()\n",
    "        for sensor in self.sensors:\n",
    "            if sensor.is_alive:\n",
    "                sensor.destroy()\n",
    "        self.sensors.clear()\n",
    "        self.vehicle = None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close environment\"\"\"\n",
    "        self._cleanup()\n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = False\n",
    "        self.world.apply_settings(settings)\n",
    "\n",
    "# ======================= DQN AGENT =======================\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim=18, action_dim=9):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = ReplayMemory(MEMORY_SIZE)\n",
    "        \n",
    "        self.epsilon = EPSILON_START\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update network\"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0.0\n",
    "        \n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = list(zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.FloatTensor(np.array(batch[0])).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch[1]).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch[2]).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch[3])).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch[4]).to(self.device)\n",
    "        \n",
    "        current_q = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_state_batch).argmax(1, keepdim=True)\n",
    "            next_q = self.target_net(next_state_batch).gather(1, next_actions).squeeze()\n",
    "            target_q = reward_batch + (1 - done_batch) * GAMMA * next_q\n",
    "        \n",
    "        loss = nn.SmoothL1Loss()(current_q.squeeze(), target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration\"\"\"\n",
    "        self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)\n",
    "\n",
    "# ======================= TRAINING =======================\n",
    "def train():\n",
    "    print(\"=\"*70)\n",
    "    print(\"CARLA DQN TRAINING - SMOOTH DRIVING + LANE ARROW FOLLOWING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    env = ImprovedCarlaEnv()\n",
    "    agent = DQNAgent(state_dim=18, action_dim=9)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_distances = []\n",
    "    episode_lengths = []\n",
    "    episode_lane_invasions = []\n",
    "    episode_lane_changes = []\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"Focus: Smooth control + Lane keeping + Following lane arrows\\n\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_avg_distance = 0.0\n",
    "    \n",
    "    for episode in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            agent.memory.push(state, action, reward, next_state, float(done))\n",
    "            loss = agent.update()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        if (episode + 1) % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_distances.append(env.distance_traveled)\n",
    "        episode_lengths.append(env.steps)\n",
    "        episode_lane_invasions.append(len(env.lane_invasion_hist))\n",
    "        episode_lane_changes.append(env.successful_lane_changes)\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_distance = np.mean(episode_distances[-10:])\n",
    "            avg_length = np.mean(episode_lengths[-10:])\n",
    "            avg_invasions = np.mean(episode_lane_invasions[-10:])\n",
    "            avg_changes = np.mean(episode_lane_changes[-10:])\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            \n",
    "            print(f\"Ep {episode+1:3d} | \"\n",
    "                  f\"R: {episode_reward:7.1f} | \"\n",
    "                  f\"D: {env.distance_traveled:6.1f}m | \"\n",
    "                  f\"L: {env.steps:3d} | \"\n",
    "                  f\"LI: {len(env.lane_invasion_hist):2d} | \"\n",
    "                  f\"LC: {env.successful_lane_changes} | \"\n",
    "                  f\"ε: {agent.epsilon:.3f} | \"\n",
    "                  f\"Avg10: R={avg_reward:6.1f}, D={avg_distance:5.1f}m, LC={avg_changes:.1f} | \"\n",
    "                  f\"T: {elapsed:.1f}min\")\n",
    "            \n",
    "            if avg_distance > best_avg_distance:\n",
    "                best_avg_distance = avg_distance\n",
    "                torch.save(agent.policy_net.state_dict(), 'carla_dqn_arrow_best.pth')\n",
    "                print(f\"  ✓ New best model saved! Avg distance: {avg_distance:.1f}m\")\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            torch.save(agent.policy_net.state_dict(), f'dqn_arrow_ep{episode+1}.pth')\n",
    "            print(f\"  ✓ Checkpoint saved\")\n",
    "    \n",
    "    torch.save(agent.policy_net.state_dict(), 'carla_dqn_arrow_final.pth')\n",
    "    print(f\"\\n✓ Training completed in {(time.time() - start_time) / 60:.1f} minutes\")\n",
    "    print(\"✓ Final model saved: carla_dqn_arrow_final.pth\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Best avg distance (10 ep): {best_avg_distance:.1f}m\")\n",
    "    print(f\"Final avg reward (last 20): {np.mean(episode_rewards[-20:]):.1f}\")\n",
    "    print(f\"Final avg distance (last 20): {np.mean(episode_distances[-20:]):.1f}m\")\n",
    "    print(f\"Final avg lane invasions (last 20): {np.mean(episode_lane_invasions[-20:]):.1f}\")\n",
    "    print(f\"Final avg successful lane changes (last 20): {np.mean(episode_lane_changes[-20:]):.1f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dqn_smooth_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "CARLA RL Training using DQN - IMPROVED VERSION\n",
    "Focus: Smooth driving with better lane keeping\n",
    "\n",
    "Key Improvements:\n",
    "1. Better action space (gradual steering changes)\n",
    "2. Enhanced state representation (heading error, curvature)\n",
    "3. Reward shaping for smooth control\n",
    "4. Action smoothing mechanism\n",
    "\"\"\"\n",
    "\n",
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "# ======================= CONFIG =======================\n",
    "EPISODES = 300\n",
    "MAX_STEPS = 500\n",
    "MEMORY_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 0.995\n",
    "LEARNING_RATE = 1e-4\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# IMPROVED DISCRETE ACTIONS (smoother, more gradual)\n",
    "# Format: (throttle, steer)\n",
    "ACTIONS = {\n",
    "    0: (0.5, 0.0),    # Straight forward (moderate speed)\n",
    "    1: (0.5, -0.15),  # Gentle left\n",
    "    2: (0.5, 0.15),   # Gentle right\n",
    "    3: (0.4, 0.0),    # Slow forward\n",
    "    4: (0.4, -0.15),  # Slow + gentle left\n",
    "    5: (0.4, 0.15),   # Slow + gentle right\n",
    "    6: (0.0, 0.0),    # Brake/Stop\n",
    "    7: (0.3, -0.3),   # Sharp left (for curves)\n",
    "    8: (0.3, 0.3),    # Sharp right (for curves)\n",
    "}\n",
    "\n",
    "# IMPROVED REWARDS\n",
    "REWARD_DISTANCE = 1.0          # Per meter forward\n",
    "REWARD_SPEED = 1.5             # For maintaining target speed\n",
    "REWARD_LANE_CENTER = 5.0       # Strong reward for staying centered\n",
    "REWARD_HEADING = 3.0           # Reward for correct heading\n",
    "REWARD_SMOOTHNESS = 2.0        # Reward for smooth control\n",
    "REWARD_COLLISION = -200.0      # Heavy collision penalty\n",
    "REWARD_OFF_ROAD = -100.0       # Off-road penalty\n",
    "REWARD_LANE_INVASION = -20.0   # Lane crossing penalty\n",
    "REWARD_JERKY = -5.0            # Penalty for sudden steering changes\n",
    "\n",
    "TARGET_SPEED = 8.0  # m/s (more reasonable for town driving)\n",
    "\n",
    "# ======================= DQN NETWORK =======================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim=15, action_dim=9):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# ======================= REPLAY MEMORY =======================\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# ======================= IMPROVED CARLA ENVIRONMENT =======================\n",
    "class ImprovedCarlaEnv:\n",
    "    def __init__(self, town='Town04', port=2000):\n",
    "        self.client = carla.Client('localhost', port)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.load_world(town)\n",
    "        \n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = True\n",
    "        settings.fixed_delta_seconds = 0.05  # Faster ticks for smoother control\n",
    "        self.world.apply_settings(settings)\n",
    "        \n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.map = self.world.get_map()\n",
    "        self.vehicle = None\n",
    "        self.sensors = []\n",
    "        \n",
    "        self.collision_hist = []\n",
    "        self.lane_invasion_hist = []\n",
    "        self.spawn_points = self.map.get_spawn_points()\n",
    "        \n",
    "        self.last_location = None\n",
    "        self.distance_traveled = 0.0\n",
    "        self.steps = 0\n",
    "        \n",
    "        # For smoothness tracking\n",
    "        self.last_steer = 0.0\n",
    "        self.last_throttle = 0.5\n",
    "        self.steer_history = deque(maxlen=5)\n",
    "        \n",
    "        print(\"Improved Environment initialized!\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self._cleanup()\n",
    "        \n",
    "        # Spawn vehicle\n",
    "        vehicle_bp = self.blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "        spawn_point = random.choice(self.spawn_points[:30])\n",
    "        \n",
    "        try:\n",
    "            self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        except:\n",
    "            spawn_point = random.choice(self.spawn_points)\n",
    "            self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        \n",
    "        # Collision sensor\n",
    "        collision_bp = self.blueprint_library.find('sensor.other.collision')\n",
    "        collision_sensor = self.world.spawn_actor(\n",
    "            collision_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        collision_sensor.listen(lambda e: self.collision_hist.append(e))\n",
    "        self.sensors.append(collision_sensor)\n",
    "        \n",
    "        # Lane invasion sensor\n",
    "        lane_bp = self.blueprint_library.find('sensor.other.lane_invasion')\n",
    "        lane_sensor = self.world.spawn_actor(\n",
    "            lane_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        lane_sensor.listen(lambda e: self.lane_invasion_hist.append(e))\n",
    "        self.sensors.append(lane_sensor)\n",
    "        \n",
    "        # Reset tracking\n",
    "        self.collision_hist.clear()\n",
    "        self.lane_invasion_hist.clear()\n",
    "        self.last_location = self.vehicle.get_location()\n",
    "        self.distance_traveled = 0.0\n",
    "        self.steps = 0\n",
    "        self.last_steer = 0.0\n",
    "        self.last_throttle = 0.5\n",
    "        self.steer_history.clear()\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            self.world.tick()\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Enhanced state representation\"\"\"\n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        waypoint = self.map.get_waypoint(\n",
    "            transform.location, \n",
    "            project_to_road=True, \n",
    "            lane_type=carla.LaneType.Driving\n",
    "        )\n",
    "        \n",
    "        if waypoint is None:\n",
    "            return np.zeros(15, dtype=np.float32)\n",
    "        \n",
    "        # Lane offset (cross-track error)\n",
    "        lane_center = waypoint.transform.location\n",
    "        offset_vec = transform.location - lane_center\n",
    "        right_vec = transform.get_right_vector()\n",
    "        lane_offset = offset_vec.x * right_vec.x + offset_vec.y * right_vec.y\n",
    "        \n",
    "        # Heading error (critical for smooth driving)\n",
    "        lane_yaw = waypoint.transform.rotation.yaw\n",
    "        vehicle_yaw = transform.rotation.yaw\n",
    "        heading_error = (vehicle_yaw - lane_yaw + 180) % 360 - 180\n",
    "        heading_error_rad = np.radians(heading_error)\n",
    "        \n",
    "        # Look-ahead waypoint for anticipation\n",
    "        next_waypoints = waypoint.next(5.0)\n",
    "        if next_waypoints:\n",
    "            next_wp = next_waypoints[0]\n",
    "            wp_vec = next_wp.transform.location - transform.location\n",
    "            forward_vec = transform.get_forward_vector()\n",
    "            \n",
    "            # Angle to next waypoint\n",
    "            angle_to_wp = np.arctan2(\n",
    "                wp_vec.y * forward_vec.x - wp_vec.x * forward_vec.y,\n",
    "                wp_vec.x * forward_vec.x + wp_vec.y * forward_vec.y\n",
    "            )\n",
    "        else:\n",
    "            angle_to_wp = 0.0\n",
    "        \n",
    "        # Road curvature (helps anticipate turns)\n",
    "        curvature = 0.0\n",
    "        if next_waypoints:\n",
    "            next_wp = next_waypoints[0]\n",
    "            curvature_yaw = next_wp.transform.rotation.yaw - lane_yaw\n",
    "            curvature = np.sin(np.radians(curvature_yaw))\n",
    "        \n",
    "        # Traffic light state\n",
    "        tl_state = 0.0  # 0: none, 0.33: green, 0.66: yellow, 1.0: red\n",
    "        if self.vehicle.is_at_traffic_light():\n",
    "            tl = self.vehicle.get_traffic_light()\n",
    "            if tl:\n",
    "                state = tl.get_state()\n",
    "                if state == carla.TrafficLightState.Green:\n",
    "                    tl_state = 0.33\n",
    "                elif state == carla.TrafficLightState.Yellow:\n",
    "                    tl_state = 0.66\n",
    "                elif state == carla.TrafficLightState.Red:\n",
    "                    tl_state = 1.0\n",
    "        \n",
    "        # Steering smoothness indicator\n",
    "        steer_variance = np.std(list(self.steer_history)) if len(self.steer_history) > 2 else 0.0\n",
    "        \n",
    "        state = np.array([\n",
    "            speed / 30.0,                          # Normalized speed\n",
    "            lane_offset / 3.5,                     # Lane offset\n",
    "            np.sin(heading_error_rad),             # Heading error (sin)\n",
    "            np.cos(heading_error_rad),             # Heading error (cos)\n",
    "            np.sin(angle_to_wp),                   # Angle to waypoint (sin)\n",
    "            np.cos(angle_to_wp),                   # Angle to waypoint (cos)\n",
    "            curvature,                             # Road curvature\n",
    "            velocity.x / 30.0,                     # Velocity X\n",
    "            velocity.y / 30.0,                     # Velocity Y\n",
    "            float(waypoint.is_junction),           # Junction flag\n",
    "            tl_state,                              # Traffic light state\n",
    "            self.last_steer,                       # Previous steering\n",
    "            steer_variance,                        # Steering smoothness\n",
    "            self.distance_traveled / 1000.0,       # Total distance\n",
    "            self.steps / 500.0                     # Normalized steps\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action_idx):\n",
    "        \"\"\"Execute action with smoothing\"\"\"\n",
    "        throttle_raw, steer_raw = ACTIONS[action_idx]\n",
    "        \n",
    "        # Apply smoothing to reduce jerky movements\n",
    "        alpha = 0.3  # Smoothing factor\n",
    "        throttle = alpha * throttle_raw + (1 - alpha) * self.last_throttle\n",
    "        steer = alpha * steer_raw + (1 - alpha) * self.last_steer\n",
    "        \n",
    "        # Limit steering rate of change\n",
    "        max_steer_change = 0.1\n",
    "        steer_change = np.clip(steer - self.last_steer, -max_steer_change, max_steer_change)\n",
    "        steer = self.last_steer + steer_change\n",
    "        \n",
    "        # Store for next iteration\n",
    "        self.last_steer = steer\n",
    "        self.last_throttle = throttle\n",
    "        self.steer_history.append(steer)\n",
    "        \n",
    "        # Apply control\n",
    "        control = carla.VehicleControl(\n",
    "            throttle=float(throttle),\n",
    "            steer=float(steer),\n",
    "            brake=0.0 if throttle > 0 else 0.5\n",
    "        )\n",
    "        \n",
    "        self.vehicle.apply_control(control)\n",
    "        self.world.tick()\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Track lane invasions\n",
    "        prev_invasions = len(self.lane_invasion_hist)\n",
    "        \n",
    "        # Update distance\n",
    "        current_location = self.vehicle.get_location()\n",
    "        distance_step = 0.0\n",
    "        if self.last_location:\n",
    "            distance_step = current_location.distance(self.last_location)\n",
    "            self.distance_traveled += distance_step\n",
    "        self.last_location = current_location\n",
    "        \n",
    "        # Get new state\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward(distance_step, prev_invasions, steer_change)\n",
    "        \n",
    "        # Check done\n",
    "        done = self._is_done()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def _calculate_reward(self, distance_step, prev_invasions, steer_change):\n",
    "        \"\"\"Improved reward function for smooth driving\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        # 1. Forward progress reward\n",
    "        reward += REWARD_DISTANCE * distance_step\n",
    "        \n",
    "        # 2. Speed reward (Gaussian around target speed)\n",
    "        speed_error = abs(speed - TARGET_SPEED)\n",
    "        speed_reward = REWARD_SPEED * np.exp(-speed_error / 5.0)\n",
    "        reward += speed_reward\n",
    "        \n",
    "        # 3. Lane keeping reward (critical!)\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        \n",
    "        if waypoint:\n",
    "            # Cross-track error\n",
    "            lane_center = waypoint.transform.location\n",
    "            offset_vec = transform.location - lane_center\n",
    "            right_vec = transform.get_right_vector()\n",
    "            lane_offset = abs(offset_vec.x * right_vec.x + offset_vec.y * right_vec.y)\n",
    "            \n",
    "            # Strong reward for staying centered (exponential decay)\n",
    "            lane_reward = REWARD_LANE_CENTER * np.exp(-lane_offset * 3.0)\n",
    "            reward += lane_reward\n",
    "            \n",
    "            # Heading alignment reward\n",
    "            lane_yaw = waypoint.transform.rotation.yaw\n",
    "            vehicle_yaw = transform.rotation.yaw\n",
    "            heading_error = abs((vehicle_yaw - lane_yaw + 180) % 360 - 180)\n",
    "            heading_reward = REWARD_HEADING * np.exp(-heading_error / 30.0)\n",
    "            reward += heading_reward\n",
    "        else:\n",
    "            reward += REWARD_OFF_ROAD\n",
    "        \n",
    "        # 4. Smoothness reward (penalize jerky steering)\n",
    "        steer_jerk = abs(steer_change)\n",
    "        if steer_jerk > 0.05:  # Penalize large steering changes\n",
    "            reward += REWARD_JERKY * steer_jerk\n",
    "        else:\n",
    "            reward += REWARD_SMOOTHNESS * (0.05 - steer_jerk)\n",
    "        \n",
    "        # 5. Lane invasion penalty\n",
    "        if len(self.lane_invasion_hist) > prev_invasions:\n",
    "            reward += REWARD_LANE_INVASION\n",
    "        \n",
    "        # 6. Collision penalty\n",
    "        if len(self.collision_hist) > 0:\n",
    "            reward += REWARD_COLLISION\n",
    "        \n",
    "        # 7. Bonus for maintaining lane over time\n",
    "        if waypoint and lane_offset < 0.5 and speed > 5.0:\n",
    "            reward += 1.0  # Bonus for good driving\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _is_done(self):\n",
    "        \"\"\"Check termination\"\"\"\n",
    "        # Collision\n",
    "        if len(self.collision_hist) > 0:\n",
    "            return True\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        \n",
    "        # Off road\n",
    "        if waypoint is None:\n",
    "            return True\n",
    "        \n",
    "        # Too far from lane center\n",
    "        if transform.location.distance(waypoint.transform.location) > 4.0:\n",
    "            return True\n",
    "        \n",
    "        # Max steps\n",
    "        if self.steps >= MAX_STEPS:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        \"\"\"Cleanup\"\"\"\n",
    "        if self.vehicle:\n",
    "            self.vehicle.destroy()\n",
    "        for sensor in self.sensors:\n",
    "            if sensor.is_alive:\n",
    "                sensor.destroy()\n",
    "        self.sensors.clear()\n",
    "        self.vehicle = None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close environment\"\"\"\n",
    "        self._cleanup()\n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = False\n",
    "        self.world.apply_settings(settings)\n",
    "\n",
    "# ======================= DQN AGENT =======================\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim=15, action_dim=9):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net = DQN(state_dim, action_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = ReplayMemory(MEMORY_SIZE)\n",
    "        \n",
    "        self.epsilon = EPSILON_START\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update network\"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample batch\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = list(zip(*transitions))\n",
    "        \n",
    "        state_batch = torch.FloatTensor(np.array(batch[0])).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch[1]).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch[2]).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(np.array(batch[3])).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch[4]).to(self.device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # Next Q values (Double DQN)\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_state_batch).argmax(1, keepdim=True)\n",
    "            next_q = self.target_net(next_state_batch).gather(1, next_actions).squeeze()\n",
    "            target_q = reward_batch + (1 - done_batch) * GAMMA * next_q\n",
    "        \n",
    "        # Huber loss (more stable than MSE)\n",
    "        loss = nn.SmoothL1Loss()(current_q.squeeze(), target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration\"\"\"\n",
    "        self.epsilon = max(EPSILON_END, self.epsilon * EPSILON_DECAY)\n",
    "\n",
    "# ======================= TRAINING =======================\n",
    "def train():\n",
    "    print(\"=\"*70)\n",
    "    print(\"IMPROVED CARLA DQN TRAINING - SMOOTH DRIVING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    env = ImprovedCarlaEnv()\n",
    "    agent = DQNAgent(state_dim=15, action_dim=9)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_distances = []\n",
    "    episode_lengths = []\n",
    "    episode_lane_invasions = []\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"Focus: Smooth control + Lane keeping\\n\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_avg_distance = 0.0\n",
    "    \n",
    "    for episode in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select and perform action\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.memory.push(state, action, reward, next_state, float(done))\n",
    "            \n",
    "            # Update network\n",
    "            loss = agent.update()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        # Update target network\n",
    "        if (episode + 1) % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_distances.append(env.distance_traveled)\n",
    "        episode_lengths.append(env.steps)\n",
    "        episode_lane_invasions.append(len(env.lane_invasion_hist))\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_distance = np.mean(episode_distances[-10:])\n",
    "            avg_length = np.mean(episode_lengths[-10:])\n",
    "            avg_invasions = np.mean(episode_lane_invasions[-10:])\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            \n",
    "            print(f\"Ep {episode+1:3d} | \"\n",
    "                  f\"R: {episode_reward:7.1f} | \"\n",
    "                  f\"D: {env.distance_traveled:6.1f}m | \"\n",
    "                  f\"L: {env.steps:3d} | \"\n",
    "                  f\"LI: {len(env.lane_invasion_hist):2d} | \"\n",
    "                  f\"ε: {agent.epsilon:.3f} | \"\n",
    "                  f\"Avg10: R={avg_reward:6.1f}, D={avg_distance:5.1f}m, LI={avg_invasions:.1f} | \"\n",
    "                  f\"T: {elapsed:.1f}min\")\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_distance > best_avg_distance:\n",
    "                best_avg_distance = avg_distance\n",
    "                torch.save(agent.policy_net.state_dict(), 'carla_dqn_smooth_best.pth')\n",
    "                print(f\"  ✓ New best model saved! Avg distance: {avg_distance:.1f}m\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            torch.save(agent.policy_net.state_dict(), f'dqn_smooth_ep{episode+1}.pth')\n",
    "            print(f\"  ✓ Checkpoint saved\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(agent.policy_net.state_dict(), 'carla_dqn_smooth_final.pth')\n",
    "    print(f\"\\n✓ Training completed in {(time.time() - start_time) / 60:.1f} minutes\")\n",
    "    print(\"✓ Final model saved: carla_dqn_smooth_final.pth\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Best avg distance (10 ep): {best_avg_distance:.1f}m\")\n",
    "    print(f\"Final avg reward (last 20): {np.mean(episode_rewards[-20:]):.1f}\")\n",
    "    print(f\"Final avg distance (last 20): {np.mean(episode_distances[-20:]):.1f}m\")\n",
    "    print(f\"Final avg lane invasions (last 20): {np.mean(episode_lane_invasions[-20:]):.1f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# carla_evlauation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dqn_lane_arrow-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CARLA DQN Evaluation Script - Lane Arrow Following Version\n",
    "Matches training with 18-dimensional state space\n",
    "Features:\n",
    "- Slower, smoother speed\n",
    "- Fixed behind-vehicle camera\n",
    "- Video generation\n",
    "- Lane arrow detection and tracking\n",
    "\"\"\"\n",
    "\n",
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "# ACTIONS - Must match training\n",
    "ACTIONS = {\n",
    "    0: (0.5, 0.0),    # Straight forward\n",
    "    1: (0.5, -0.15),  # Gentle left\n",
    "    2: (0.5, 0.15),   # Gentle right\n",
    "    3: (0.4, 0.0),    # Slow forward\n",
    "    4: (0.4, -0.15),  # Slow + gentle left\n",
    "    5: (0.4, 0.15),   # Slow + gentle right\n",
    "    6: (0.0, 0.0),    # Brake/Stop\n",
    "    7: (0.3, -0.3),   # Sharp left (for lane changes)\n",
    "    8: (0.3, 0.3),    # Sharp right (for lane changes)\n",
    "}\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim=18, action_dim=9):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # MUST match training architecture (with dropout)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class CarlaEvalEnv:\n",
    "    def __init__(self, visualize=True):\n",
    "        self.client = carla.Client('localhost', 2000)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.load_world('Town04')\n",
    "        \n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = True\n",
    "        settings.fixed_delta_seconds = 0.05\n",
    "        self.world.apply_settings(settings)\n",
    "        \n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.map = self.world.get_map()\n",
    "        self.vehicle = None\n",
    "        self.sensors = []\n",
    "        self.camera = None\n",
    "        self.visualize = visualize\n",
    "        \n",
    "        self.collision_hist = []\n",
    "        self.lane_invasion_hist = []\n",
    "        self.spawn_points = self.map.get_spawn_points()\n",
    "        \n",
    "        self.distance_traveled = 0.0\n",
    "        self.last_location = None\n",
    "        self.steps = 0\n",
    "        self.speeds = []\n",
    "        self.lane_offsets = []\n",
    "        self.traffic_light_encounters = []\n",
    "        \n",
    "        # Video recording\n",
    "        self.latest_image = None\n",
    "        self.recording_frames = []\n",
    "        \n",
    "        # For state tracking\n",
    "        self.last_steer = 0.0\n",
    "        \n",
    "        # Lane change tracking\n",
    "        self.current_lane_id = None\n",
    "        self.target_lane_from_arrow = None\n",
    "        self.lane_change_in_progress = False\n",
    "        self.successful_lane_changes = 0\n",
    "        self.lane_arrows_detected = []\n",
    "    \n",
    "    def camera_callback(self, image):\n",
    "        \"\"\"Callback to capture camera frames for video\"\"\"\n",
    "        try:\n",
    "            arr = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "            arr = arr.reshape((image.height, image.width, 4))[:, :, :3]\n",
    "            arr = arr[:, :, ::-1]  # RGB to BGR\n",
    "            self.latest_image = arr.copy()\n",
    "            self.recording_frames.append(arr.copy())\n",
    "        except Exception as e:\n",
    "            print(f\"Camera callback error: {e}\")\n",
    "    \n",
    "    def _detect_lane_arrow(self, waypoint):\n",
    "        \"\"\"Detect if current lane has left/right arrow marking\"\"\"\n",
    "        if waypoint is None:\n",
    "            return None\n",
    "        \n",
    "        lane_change = waypoint.lane_change\n",
    "        \n",
    "        if lane_change == carla.LaneChange.Left:\n",
    "            return \"left\"\n",
    "        elif lane_change == carla.LaneChange.Right:\n",
    "            return \"right\"\n",
    "        elif lane_change == carla.LaneChange.Both:\n",
    "            return \"both\"\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _check_lane_availability(self, waypoint, direction):\n",
    "        \"\"\"Check if lane change to direction is safe and valid\"\"\"\n",
    "        if waypoint is None or direction is None:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if direction == \"left\":\n",
    "                target_wp = waypoint.get_left_lane()\n",
    "            elif direction == \"right\":\n",
    "                target_wp = waypoint.get_right_lane()\n",
    "            else:\n",
    "                return False\n",
    "            \n",
    "            if target_wp is None:\n",
    "                return False\n",
    "            \n",
    "            if target_wp.lane_type != carla.LaneType.Driving:\n",
    "                return False\n",
    "            \n",
    "            if (waypoint.lane_id > 0) != (target_wp.lane_id > 0):\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def reset(self, spawn_idx=0):\n",
    "        self._cleanup()\n",
    "        \n",
    "        vehicle_bp = self.blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "        spawn_point = self.spawn_points[spawn_idx % len(self.spawn_points)]\n",
    "        self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        \n",
    "        # Collision sensor\n",
    "        collision_bp = self.blueprint_library.find('sensor.other.collision')\n",
    "        collision_sensor = self.world.spawn_actor(\n",
    "            collision_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        collision_sensor.listen(lambda e: self.collision_hist.append(e))\n",
    "        self.sensors.append(collision_sensor)\n",
    "        \n",
    "        # Lane invasion sensor\n",
    "        lane_bp = self.blueprint_library.find('sensor.other.lane_invasion')\n",
    "        lane_sensor = self.world.spawn_actor(\n",
    "            lane_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        lane_sensor.listen(lambda e: self.lane_invasion_hist.append(e))\n",
    "        self.sensors.append(lane_sensor)\n",
    "        \n",
    "        # Camera for recording (mounted behind vehicle)\n",
    "        cam_bp = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        cam_bp.set_attribute('image_size_x', '800')\n",
    "        cam_bp.set_attribute('image_size_y', '600')\n",
    "        cam_bp.set_attribute('fov', '90')\n",
    "        cam_transform = carla.Transform(\n",
    "            carla.Location(x=-5.0, z=2.5),\n",
    "            carla.Rotation(pitch=-10)\n",
    "        )\n",
    "        self.camera = self.world.spawn_actor(cam_bp, cam_transform, attach_to=self.vehicle)\n",
    "        self.camera.listen(self.camera_callback)\n",
    "        self.sensors.append(self.camera)\n",
    "        \n",
    "        self.collision_hist.clear()\n",
    "        self.lane_invasion_hist.clear()\n",
    "        self.distance_traveled = 0.0\n",
    "        self.last_location = self.vehicle.get_location()\n",
    "        self.steps = 0\n",
    "        self.speeds = []\n",
    "        self.lane_offsets = []\n",
    "        self.traffic_light_encounters = []\n",
    "        self.recording_frames = []\n",
    "        self.last_steer = 0.0\n",
    "        \n",
    "        # Reset lane tracking\n",
    "        self.current_lane_id = None\n",
    "        self.target_lane_from_arrow = None\n",
    "        self.lane_change_in_progress = False\n",
    "        self.successful_lane_changes = 0\n",
    "        self.lane_arrows_detected = []\n",
    "        \n",
    "        if self.visualize:\n",
    "            spectator = self.world.get_spectator()\n",
    "            t = self.vehicle.get_transform()\n",
    "            spectator.set_transform(carla.Transform(\n",
    "                t.location + carla.Location(x=-10, z=5),\n",
    "                carla.Rotation(pitch=-20, yaw=t.rotation.yaw)\n",
    "            ))\n",
    "        \n",
    "        for _ in range(10):\n",
    "            self.world.tick()\n",
    "        \n",
    "        # Initialize lane ID\n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        if waypoint:\n",
    "            self.current_lane_id = waypoint.lane_id\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Enhanced state representation - 18 dimensions\"\"\"\n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        \n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        self.speeds.append(speed)\n",
    "        \n",
    "        waypoint = self.map.get_waypoint(\n",
    "            transform.location, project_to_road=True, lane_type=carla.LaneType.Driving\n",
    "        )\n",
    "        \n",
    "        if waypoint is None:\n",
    "            return np.zeros(18, dtype=np.float32)\n",
    "        \n",
    "        # Lane offset (cross-track error)\n",
    "        lane_center = waypoint.transform.location\n",
    "        offset_vec = transform.location - lane_center\n",
    "        right_vec = transform.get_right_vector()\n",
    "        lane_offset = offset_vec.x * right_vec.x + offset_vec.y * right_vec.y\n",
    "        self.lane_offsets.append(abs(lane_offset))\n",
    "        \n",
    "        # Heading error\n",
    "        lane_yaw = waypoint.transform.rotation.yaw\n",
    "        vehicle_yaw = transform.rotation.yaw\n",
    "        heading_error = (vehicle_yaw - lane_yaw + 180) % 360 - 180\n",
    "        heading_error_rad = np.radians(heading_error)\n",
    "        \n",
    "        # Look-ahead waypoint\n",
    "        next_waypoints = waypoint.next(5.0)\n",
    "        if next_waypoints:\n",
    "            next_wp = next_waypoints[0]\n",
    "            wp_vec = next_wp.transform.location - transform.location\n",
    "            forward_vec = transform.get_forward_vector()\n",
    "            angle_to_wp = np.arctan2(\n",
    "                wp_vec.y * forward_vec.x - wp_vec.x * forward_vec.y,\n",
    "                wp_vec.x * forward_vec.x + wp_vec.y * forward_vec.y\n",
    "            )\n",
    "        else:\n",
    "            angle_to_wp = 0.0\n",
    "        \n",
    "        # Road curvature\n",
    "        curvature = 0.0\n",
    "        if next_waypoints:\n",
    "            next_wp = next_waypoints[0]\n",
    "            curvature_yaw = next_wp.transform.rotation.yaw - lane_yaw\n",
    "            curvature = np.sin(np.radians(curvature_yaw))\n",
    "        \n",
    "        # Traffic light state\n",
    "        tl_state = 0.0\n",
    "        if self.vehicle.is_at_traffic_light():\n",
    "            tl = self.vehicle.get_traffic_light()\n",
    "            if tl:\n",
    "                state_val = tl.get_state()\n",
    "                self.traffic_light_encounters.append({\n",
    "                    'state': str(state_val),\n",
    "                    'speed': speed,\n",
    "                    'step': self.steps\n",
    "                })\n",
    "                if state_val == carla.TrafficLightState.Green:\n",
    "                    tl_state = 0.33\n",
    "                elif state_val == carla.TrafficLightState.Yellow:\n",
    "                    tl_state = 0.66\n",
    "                elif state_val == carla.TrafficLightState.Red:\n",
    "                    tl_state = 1.0\n",
    "        \n",
    "        # Steering smoothness\n",
    "        steer_variance = 0.0\n",
    "        \n",
    "        # Lane arrow detection\n",
    "        lane_arrow = self._detect_lane_arrow(waypoint)\n",
    "        \n",
    "        # Encode lane arrow\n",
    "        arrow_left = 1.0 if lane_arrow in [\"left\", \"both\"] else 0.0\n",
    "        arrow_right = 1.0 if lane_arrow in [\"right\", \"both\"] else 0.0\n",
    "        arrow_straight = 1.0 if lane_arrow is None else 0.0\n",
    "        \n",
    "        # Check lane availability\n",
    "        can_change_left = 1.0 if self._check_lane_availability(waypoint, \"left\") else 0.0\n",
    "        can_change_right = 1.0 if self._check_lane_availability(waypoint, \"right\") else 0.0\n",
    "        \n",
    "        # Log arrow detection\n",
    "        if lane_arrow is not None and lane_arrow not in [d['direction'] for d in self.lane_arrows_detected[-3:]]:\n",
    "            self.lane_arrows_detected.append({\n",
    "                'direction': lane_arrow,\n",
    "                'step': self.steps,\n",
    "                'location': (transform.location.x, transform.location.y)\n",
    "            })\n",
    "        \n",
    "        # Update target from arrow\n",
    "        if not self.lane_change_in_progress:\n",
    "            if arrow_left and can_change_left:\n",
    "                self.target_lane_from_arrow = \"left\"\n",
    "            elif arrow_right and can_change_right:\n",
    "                self.target_lane_from_arrow = \"right\"\n",
    "            else:\n",
    "                self.target_lane_from_arrow = None\n",
    "        \n",
    "        state = np.array([\n",
    "            speed / 30.0,                          # 0: Normalized speed\n",
    "            lane_offset / 3.5,                     # 1: Lane offset\n",
    "            np.sin(heading_error_rad),             # 2: Heading error (sin)\n",
    "            np.cos(heading_error_rad),             # 3: Heading error (cos)\n",
    "            np.sin(angle_to_wp),                   # 4: Angle to waypoint (sin)\n",
    "            np.cos(angle_to_wp),                   # 5: Angle to waypoint (cos)\n",
    "            curvature,                             # 6: Road curvature\n",
    "            velocity.x / 30.0,                     # 7: Velocity X\n",
    "            velocity.y / 30.0,                     # 8: Velocity Y\n",
    "            float(waypoint.is_junction),           # 9: Junction flag\n",
    "            tl_state,                              # 10: Traffic light state\n",
    "            self.last_steer,                       # 11: Previous steering\n",
    "            steer_variance,                        # 12: Steering smoothness\n",
    "            arrow_left,                            # 13: Lane arrow left\n",
    "            arrow_right,                           # 14: Lane arrow right\n",
    "            arrow_straight,                        # 15: No arrow (straight)\n",
    "            can_change_left,                       # 16: Left lane available\n",
    "            can_change_right,                      # 17: Right lane available\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action_idx):\n",
    "        throttle, steer = ACTIONS[action_idx]\n",
    "        \n",
    "        # Store for state tracking\n",
    "        self.last_steer = steer\n",
    "        \n",
    "        control = carla.VehicleControl(\n",
    "            throttle=throttle,\n",
    "            steer=steer,\n",
    "            brake=0.0 if throttle > 0 else 0.3\n",
    "        )\n",
    "        \n",
    "        self.vehicle.apply_control(control)\n",
    "        \n",
    "        if self.visualize:\n",
    "            spectator = self.world.get_spectator()\n",
    "            t = self.vehicle.get_transform()\n",
    "            spec_location = t.location - t.get_forward_vector() * 10 + carla.Location(z=5)\n",
    "            spectator.set_transform(carla.Transform(\n",
    "                spec_location,\n",
    "                carla.Rotation(pitch=-20, yaw=t.rotation.yaw)\n",
    "            ))\n",
    "        \n",
    "        self.world.tick()\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Track lane changes\n",
    "        prev_lane_id = self.current_lane_id\n",
    "        \n",
    "        current_location = self.vehicle.get_location()\n",
    "        if self.last_location:\n",
    "            self.distance_traveled += current_location.distance(self.last_location)\n",
    "        self.last_location = current_location\n",
    "        \n",
    "        # Update current lane\n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        if waypoint:\n",
    "            self.current_lane_id = waypoint.lane_id\n",
    "        \n",
    "        # Detect lane change completion\n",
    "        if prev_lane_id is not None and self.current_lane_id != prev_lane_id:\n",
    "            if self.current_lane_id > prev_lane_id:\n",
    "                lane_change_direction = \"left\"\n",
    "            else:\n",
    "                lane_change_direction = \"right\"\n",
    "            \n",
    "            if lane_change_direction == self.target_lane_from_arrow:\n",
    "                self.successful_lane_changes += 1\n",
    "                print(f\"    ✓ Lane change {lane_change_direction} at step {self.steps}\")\n",
    "            \n",
    "            self.lane_change_in_progress = False\n",
    "            self.target_lane_from_arrow = None\n",
    "        \n",
    "        # Detect lane change initiation\n",
    "        if not self.lane_change_in_progress and abs(steer) > 0.2:\n",
    "            self.lane_change_in_progress = True\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        done = self._is_done()\n",
    "        \n",
    "        return next_state, done\n",
    "    \n",
    "    def _is_done(self):\n",
    "        if len(self.collision_hist) > 0:\n",
    "            return True\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        \n",
    "        if waypoint is None:\n",
    "            return True\n",
    "        \n",
    "        if transform.location.distance(waypoint.transform.location) > 4.0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def save_video(self, episode_num, output_dir=\"eval_videos\"):\n",
    "        \"\"\"Save recorded frames as video\"\"\"\n",
    "        if not self.recording_frames:\n",
    "            print(f\"  No frames to save for episode {episode_num}\")\n",
    "            return None\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        video_path = os.path.join(output_dir, f\"episode_{episode_num}.mp4\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"  Creating video from {len(self.recording_frames)} frames...\")\n",
    "            with imageio.get_writer(video_path, fps=20) as writer:\n",
    "                for frame in self.recording_frames:\n",
    "                    writer.append_data(frame)\n",
    "            print(f\"  ✓ Video saved: {video_path}\")\n",
    "            return video_path\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Video creation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        if self.vehicle:\n",
    "            self.vehicle.destroy()\n",
    "        for sensor in self.sensors:\n",
    "            if sensor.is_alive:\n",
    "                sensor.destroy()\n",
    "        self.sensors.clear()\n",
    "        self.vehicle = None\n",
    "        self.camera = None\n",
    "    \n",
    "    def close(self):\n",
    "        self._cleanup()\n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = False\n",
    "        self.world.apply_settings(settings)\n",
    "\n",
    "def evaluate(model_path, num_episodes=5, max_steps=1000):\n",
    "    print(\"=\"*70)\n",
    "    print(\"CARLA DQN EVALUATION - LANE ARROW FOLLOWING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DQN(state_dim=18, action_dim=9).to(device)  # 18 dims for arrow following\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "        print(f\"✓ Model loaded: {model_path}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load model: {e}\")\n",
    "        return\n",
    "    \n",
    "    env = CarlaEvalEnv(visualize=True)\n",
    "    \n",
    "    all_distances = []\n",
    "    all_speeds = []\n",
    "    all_collisions = 0\n",
    "    all_lane_invasions = 0\n",
    "    all_lane_changes = 0\n",
    "    all_arrows_detected = 0\n",
    "    video_paths = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"EPISODE {ep+1}/{num_episodes}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        state = env.reset(spawn_idx=ep)\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        print(\"Running... (Recording video + Tracking lane arrows)\")\n",
    "        print(\"Speed: SMOOTH for better control\\n\")\n",
    "        action_counts = {i: 0 for i in range(9)}\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_tensor)\n",
    "                action = q_values.argmax().item()\n",
    "            \n",
    "            action_counts[action] += 1\n",
    "            next_state, done = env.step(action)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if steps % 50 == 0:\n",
    "                avg_speed = np.mean(env.speeds[-50:])\n",
    "                avg_lane = np.mean(env.lane_offsets[-50:])\n",
    "                print(f\"  Step {steps:4d}: Speed={avg_speed:4.1f} m/s | \"\n",
    "                      f\"Distance={env.distance_traveled:6.1f}m | \"\n",
    "                      f\"Lane offset={avg_lane:4.2f}m | \"\n",
    "                      f\"Lane changes={env.successful_lane_changes}\")\n",
    "        \n",
    "        # Episode summary\n",
    "        avg_speed = np.mean(env.speeds) if env.speeds else 0\n",
    "        avg_lane_offset = np.mean(env.lane_offsets) if env.lane_offsets else 0\n",
    "        max_lane_offset = max(env.lane_offsets) if env.lane_offsets else 0\n",
    "        \n",
    "        all_distances.append(env.distance_traveled)\n",
    "        all_speeds.append(avg_speed)\n",
    "        if len(env.collision_hist) > 0:\n",
    "            all_collisions += 1\n",
    "        all_lane_invasions += len(env.lane_invasion_hist)\n",
    "        all_lane_changes += env.successful_lane_changes\n",
    "        all_arrows_detected += len(env.lane_arrows_detected)\n",
    "        \n",
    "        print(f\"\\n{'─'*70}\")\n",
    "        print(f\"EPISODE {ep+1} SUMMARY\")\n",
    "        print(f\"{'─'*70}\")\n",
    "        print(f\"Outcome: {'✗ COLLISION' if len(env.collision_hist) > 0 else '✓ SUCCESS'}\")\n",
    "        print(f\"\\nDriving Performance:\")\n",
    "        print(f\"  • Distance Traveled: {env.distance_traveled:.1f} m\")\n",
    "        print(f\"  • Average Speed: {avg_speed:.1f} m/s ({avg_speed*3.6:.1f} km/h)\")\n",
    "        print(f\"  • Episode Duration: {steps} steps ({steps*0.05:.1f} seconds)\")\n",
    "        print(f\"\\nLane Keeping:\")\n",
    "        print(f\"  • Average Lane Offset: {avg_lane_offset:.3f} m\")\n",
    "        print(f\"  • Maximum Lane Offset: {max_lane_offset:.3f} m\")\n",
    "        print(f\"  • Lane Invasions: {len(env.lane_invasion_hist)}\")\n",
    "        print(f\"\\nLane Arrow Following:\")\n",
    "        print(f\"  • Arrows Detected: {len(env.lane_arrows_detected)}\")\n",
    "        print(f\"  • Successful Lane Changes: {env.successful_lane_changes}\")\n",
    "        if env.lane_arrows_detected:\n",
    "            print(f\"  • Arrow Locations:\")\n",
    "            for arrow in env.lane_arrows_detected:\n",
    "                print(f\"    - {arrow['direction'].upper()} at step {arrow['step']}\")\n",
    "        print(f\"\\nSafety:\")\n",
    "        print(f\"  • Collisions: {len(env.collision_hist)}\")\n",
    "        \n",
    "        print(f\"\\nAction Distribution:\")\n",
    "        action_names = ['STRAIGHT', 'GENTLE_L', 'GENTLE_R', 'SLOW', 'SLOW_L', 'SLOW_R', 'BRAKE', 'SHARP_L', 'SHARP_R']\n",
    "        for i, name in enumerate(action_names):\n",
    "            pct = 100 * action_counts[i] / steps if steps > 0 else 0\n",
    "            if pct > 1.0:  # Only show actions used >1%\n",
    "                bar = '█' * int(pct / 2)\n",
    "                print(f\"  {name:10s}: {action_counts[i]:4d} ({pct:5.1f}%) {bar}\")\n",
    "        \n",
    "        # Save video\n",
    "        video_path = env.save_video(ep + 1)\n",
    "        if video_path:\n",
    "            video_paths.append(video_path)\n",
    "        \n",
    "        print()\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Overall summary\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"OVERALL EVALUATION RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    success_rate = (num_episodes - all_collisions) / num_episodes\n",
    "    avg_distance = np.mean(all_distances)\n",
    "    avg_speed = np.mean(all_speeds)\n",
    "    \n",
    "    print(f\"\\nSuccess Rate: {success_rate*100:.1f}% ({num_episodes - all_collisions}/{num_episodes} episodes)\")\n",
    "    print(f\"Average Distance: {avg_distance:.1f} m\")\n",
    "    print(f\"Average Speed: {avg_speed:.1f} m/s ({avg_speed*3.6:.1f} km/h)\")\n",
    "    print(f\"Total Collisions: {all_collisions}\")\n",
    "    print(f\"Total Lane Invasions: {all_lane_invasions}\")\n",
    "    print(f\"\\nLane Arrow Following Performance:\")\n",
    "    print(f\"Total Arrows Detected: {all_arrows_detected}\")\n",
    "    print(f\"Total Successful Lane Changes: {all_lane_changes}\")\n",
    "    if all_arrows_detected > 0:\n",
    "        print(f\"Lane Change Success Rate: {(all_lane_changes / all_arrows_detected) * 100:.1f}%\")\n",
    "    \n",
    "    # Video summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"VIDEO RECORDINGS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Videos saved: {len(video_paths)}/{num_episodes}\")\n",
    "    for vp in video_paths:\n",
    "        print(f\"  • {vp}\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PERFORMANCE ASSESSMENT\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    speed_score = min(100, (avg_speed / 7.0) * 100)\n",
    "    safety_score = success_rate * 100\n",
    "    distance_score = min(100, (avg_distance / 500.0) * 100)\n",
    "    \n",
    "    # Lane arrow following score\n",
    "    arrow_score = 0.0\n",
    "    if all_arrows_detected > 0:\n",
    "        arrow_score = min(100, (all_lane_changes / all_arrows_detected) * 100)\n",
    "    \n",
    "    overall_score = (speed_score + safety_score + distance_score + arrow_score) / 4\n",
    "    \n",
    "    print(f\"\\nSpeed Score:       {speed_score:5.1f}% (target: 7+ m/s)\")\n",
    "    print(f\"Safety Score:      {safety_score:5.1f}% (no collisions)\")\n",
    "    print(f\"Distance Score:    {distance_score:5.1f}% (target: 500+ m)\")\n",
    "    print(f\"Arrow Follow Score: {arrow_score:5.1f}% (follow lane arrows)\")\n",
    "    print(f\"Overall Score:     {overall_score:5.1f}%\")\n",
    "    \n",
    "    if overall_score >= 85:\n",
    "        grade = \"A - Excellent autonomous driving with lane following!\"\n",
    "    elif overall_score >= 75:\n",
    "        grade = \"B - Good driving, minor improvements needed\"\n",
    "    elif overall_score >= 65:\n",
    "        grade = \"C - Fair, needs better arrow following\"\n",
    "    elif overall_score >= 50:\n",
    "        grade = \"D - Poor, inconsistent behavior\"\n",
    "    else:\n",
    "        grade = \"F - Failed to demonstrate autonomous driving\"\n",
    "    \n",
    "    print(f\"\\nFINAL GRADE: {grade}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', default='carla_dqn_arrow_best.pth',\n",
    "                       help='Path to trained model (default: carla_dqn_arrow_best.pth)')\n",
    "    parser.add_argument('--episodes', type=int, default=5,\n",
    "                       help='Number of episodes to evaluate (default: 5)')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"\\nMake sure CARLA is running!\\n\")\n",
    "    print(\"This evaluation tracks lane arrow following behavior.\")\n",
    "    print(\"The agent should change lanes when arrows indicate.\\n\")\n",
    "    input(\"Press Enter to start...\")\n",
    "    \n",
    "    evaluate(args.model, args.episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smooth_lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CARLA DQN Evaluation Script - Modified Version\n",
    "Changes:\n",
    "- Slower, smoother speed\n",
    "- Fixed behind-vehicle camera\n",
    "- Video generation added\n",
    "\"\"\"\n",
    "\n",
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "# IMPROVED DISCRETE ACTIONS (matches training - smoother control)\n",
    "# Format: (throttle, steer)\n",
    "ACTIONS = {\n",
    "    0: (0.5, 0.0),    # Straight forward (moderate speed)\n",
    "    1: (0.5, -0.15),  # Gentle left\n",
    "    2: (0.5, 0.15),   # Gentle right\n",
    "    3: (0.4, 0.0),    # Slow forward\n",
    "    4: (0.4, -0.15),  # Slow + gentle left\n",
    "    5: (0.4, 0.15),   # Slow + gentle right\n",
    "    6: (0.0, 0.0),    # Brake/Stop\n",
    "    7: (0.3, -0.3),   # Sharp left (for curves)\n",
    "    8: (0.3, 0.3),    # Sharp right (for curves)\n",
    "}\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim=15, action_dim=9):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # MUST match training architecture (with dropout)\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class CarlaEvalEnv:\n",
    "    def __init__(self, visualize=True):\n",
    "        self.client = carla.Client('localhost', 2000)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.load_world('Town04')\n",
    "        \n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = True\n",
    "        settings.fixed_delta_seconds = 0.05\n",
    "        self.world.apply_settings(settings)\n",
    "        \n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.map = self.world.get_map()\n",
    "        self.vehicle = None\n",
    "        self.sensors = []\n",
    "        self.camera = None  # For video capture\n",
    "        self.visualize = visualize\n",
    "        \n",
    "        self.collision_hist = []\n",
    "        self.lane_invasion_hist = []\n",
    "        self.spawn_points = self.map.get_spawn_points()\n",
    "        \n",
    "        self.distance_traveled = 0.0\n",
    "        self.last_location = None\n",
    "        self.steps = 0\n",
    "        self.speeds = []\n",
    "        self.lane_offsets = []\n",
    "        self.traffic_light_encounters = []\n",
    "        \n",
    "        # Video recording\n",
    "        self.latest_image = None\n",
    "        self.recording_frames = []\n",
    "        \n",
    "        # For state tracking\n",
    "        self.last_steer = 0.0\n",
    "    \n",
    "    def camera_callback(self, image):\n",
    "        \"\"\"Callback to capture camera frames for video\"\"\"\n",
    "        try:\n",
    "            arr = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "            arr = arr.reshape((image.height, image.width, 4))[:, :, :3]\n",
    "            arr = arr[:, :, ::-1]  # RGB to BGR\n",
    "            self.latest_image = arr.copy()\n",
    "            self.recording_frames.append(arr.copy())\n",
    "        except Exception as e:\n",
    "            print(f\"Camera callback error: {e}\")\n",
    "    \n",
    "    def reset(self, spawn_idx=0):\n",
    "        self._cleanup()\n",
    "        \n",
    "        vehicle_bp = self.blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "        spawn_point = self.spawn_points[spawn_idx % len(self.spawn_points)]\n",
    "        self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        \n",
    "        # Collision sensor\n",
    "        collision_bp = self.blueprint_library.find('sensor.other.collision')\n",
    "        collision_sensor = self.world.spawn_actor(\n",
    "            collision_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        collision_sensor.listen(lambda e: self.collision_hist.append(e))\n",
    "        self.sensors.append(collision_sensor)\n",
    "        \n",
    "        # Lane invasion sensor\n",
    "        lane_bp = self.blueprint_library.find('sensor.other.lane_invasion')\n",
    "        lane_sensor = self.world.spawn_actor(\n",
    "            lane_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        lane_sensor.listen(lambda e: self.lane_invasion_hist.append(e))\n",
    "        self.sensors.append(lane_sensor)\n",
    "        \n",
    "        # Camera for recording (mounted behind vehicle)\n",
    "        cam_bp = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        cam_bp.set_attribute('image_size_x', '800')\n",
    "        cam_bp.set_attribute('image_size_y', '600')\n",
    "        cam_bp.set_attribute('fov', '90')\n",
    "        # Position: 5m behind, 2.5m up, slight downward pitch\n",
    "        cam_transform = carla.Transform(\n",
    "            carla.Location(x=-5.0, z=2.5),\n",
    "            carla.Rotation(pitch=-10)\n",
    "        )\n",
    "        self.camera = self.world.spawn_actor(cam_bp, cam_transform, attach_to=self.vehicle)\n",
    "        self.camera.listen(self.camera_callback)\n",
    "        self.sensors.append(self.camera)\n",
    "        \n",
    "        self.collision_hist.clear()\n",
    "        self.lane_invasion_hist.clear()\n",
    "        self.distance_traveled = 0.0\n",
    "        self.last_location = self.vehicle.get_location()\n",
    "        self.steps = 0\n",
    "        self.speeds = []\n",
    "        self.lane_offsets = []\n",
    "        self.traffic_light_encounters = []\n",
    "        self.recording_frames = []\n",
    "        self.last_steer = 0.0\n",
    "        \n",
    "        if self.visualize:\n",
    "            spectator = self.world.get_spectator()\n",
    "            t = self.vehicle.get_transform()\n",
    "            # FIXED: Spectator directly behind vehicle\n",
    "            spectator.set_transform(carla.Transform(\n",
    "                t.location + carla.Location(x=-10, z=5),\n",
    "                carla.Rotation(pitch=-20, yaw=t.rotation.yaw)\n",
    "            ))\n",
    "        \n",
    "        for _ in range(10):\n",
    "            self.world.tick()\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Enhanced state representation - MUST match training (15 dims)\"\"\"\n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        \n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        self.speeds.append(speed)\n",
    "        \n",
    "        waypoint = self.map.get_waypoint(\n",
    "            transform.location, project_to_road=True, lane_type=carla.LaneType.Driving\n",
    "        )\n",
    "        \n",
    "        if waypoint is None:\n",
    "            return np.zeros(15, dtype=np.float32)\n",
    "        \n",
    "        # Lane offset (cross-track error)\n",
    "        lane_center = waypoint.transform.location\n",
    "        offset_vec = transform.location - lane_center\n",
    "        right_vec = transform.get_right_vector()\n",
    "        lane_offset = offset_vec.x * right_vec.x + offset_vec.y * right_vec.y\n",
    "        self.lane_offsets.append(abs(lane_offset))\n",
    "        \n",
    "        # Heading error (critical for smooth driving)\n",
    "        lane_yaw = waypoint.transform.rotation.yaw\n",
    "        vehicle_yaw = transform.rotation.yaw\n",
    "        heading_error = (vehicle_yaw - lane_yaw + 180) % 360 - 180\n",
    "        heading_error_rad = np.radians(heading_error)\n",
    "        \n",
    "        # Look-ahead waypoint for anticipation\n",
    "        next_waypoints = waypoint.next(5.0)\n",
    "        if next_waypoints:\n",
    "            next_wp = next_waypoints[0]\n",
    "            wp_vec = next_wp.transform.location - transform.location\n",
    "            forward_vec = transform.get_forward_vector()\n",
    "            \n",
    "            # Angle to next waypoint\n",
    "            angle_to_wp = np.arctan2(\n",
    "                wp_vec.y * forward_vec.x - wp_vec.x * forward_vec.y,\n",
    "                wp_vec.x * forward_vec.x + wp_vec.y * forward_vec.y\n",
    "            )\n",
    "        else:\n",
    "            angle_to_wp = 0.0\n",
    "        \n",
    "        # Road curvature (helps anticipate turns)\n",
    "        curvature = 0.0\n",
    "        if next_waypoints:\n",
    "            next_wp = next_waypoints[0]\n",
    "            curvature_yaw = next_wp.transform.rotation.yaw - lane_yaw\n",
    "            curvature = np.sin(np.radians(curvature_yaw))\n",
    "        \n",
    "        # Traffic light state\n",
    "        tl_state = 0.0  # 0: none, 0.33: green, 0.66: yellow, 1.0: red\n",
    "        if self.vehicle.is_at_traffic_light():\n",
    "            tl = self.vehicle.get_traffic_light()\n",
    "            if tl:\n",
    "                state_val = tl.get_state()\n",
    "                self.traffic_light_encounters.append({\n",
    "                    'state': str(state_val),\n",
    "                    'speed': speed,\n",
    "                    'step': self.steps\n",
    "                })\n",
    "                if state_val == carla.TrafficLightState.Green:\n",
    "                    tl_state = 0.33\n",
    "                elif state_val == carla.TrafficLightState.Yellow:\n",
    "                    tl_state = 0.66\n",
    "                elif state_val == carla.TrafficLightState.Red:\n",
    "                    tl_state = 1.0\n",
    "        \n",
    "        # Steering smoothness (use last steer or default)\n",
    "        last_steer = getattr(self, 'last_steer', 0.0)\n",
    "        steer_variance = 0.0  # Can't compute in evaluation mode without history\n",
    "        \n",
    "        state = np.array([\n",
    "            speed / 30.0,                          # 0: Normalized speed\n",
    "            lane_offset / 3.5,                     # 1: Lane offset\n",
    "            np.sin(heading_error_rad),             # 2: Heading error (sin)\n",
    "            np.cos(heading_error_rad),             # 3: Heading error (cos)\n",
    "            np.sin(angle_to_wp),                   # 4: Angle to waypoint (sin)\n",
    "            np.cos(angle_to_wp),                   # 5: Angle to waypoint (cos)\n",
    "            curvature,                             # 6: Road curvature\n",
    "            velocity.x / 30.0,                     # 7: Velocity X\n",
    "            velocity.y / 30.0,                     # 8: Velocity Y\n",
    "            float(waypoint.is_junction),           # 9: Junction flag\n",
    "            tl_state,                              # 10: Traffic light state\n",
    "            last_steer,                            # 11: Previous steering\n",
    "            steer_variance,                        # 12: Steering smoothness\n",
    "            self.distance_traveled / 1000.0,       # 13: Total distance\n",
    "            self.steps / 500.0                     # 14: Normalized steps\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action_idx):\n",
    "        throttle, steer = ACTIONS[action_idx]\n",
    "        \n",
    "        # Store for state tracking\n",
    "        self.last_steer = steer\n",
    "        \n",
    "        control = carla.VehicleControl(\n",
    "            throttle=throttle,\n",
    "            steer=steer,\n",
    "            brake=0.0 if throttle > 0 else 0.3\n",
    "        )\n",
    "        \n",
    "        self.vehicle.apply_control(control)\n",
    "        \n",
    "        if self.visualize:\n",
    "            spectator = self.world.get_spectator()\n",
    "            t = self.vehicle.get_transform()\n",
    "            # FIXED: Keep spectator directly behind, no zigzag\n",
    "            spec_location = t.location - t.get_forward_vector() * 10 + carla.Location(z=5)\n",
    "            spectator.set_transform(carla.Transform(\n",
    "                spec_location,\n",
    "                carla.Rotation(pitch=-20, yaw=t.rotation.yaw)\n",
    "            ))\n",
    "        \n",
    "        self.world.tick()\n",
    "        self.steps += 1\n",
    "        \n",
    "        current_location = self.vehicle.get_location()\n",
    "        if self.last_location:\n",
    "            self.distance_traveled += current_location.distance(self.last_location)\n",
    "        self.last_location = current_location\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        done = self._is_done()\n",
    "        \n",
    "        return next_state, done\n",
    "    \n",
    "    def _is_done(self):\n",
    "        if len(self.collision_hist) > 0:\n",
    "            return True\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        \n",
    "        if waypoint is None:\n",
    "            return True\n",
    "        \n",
    "        if transform.location.distance(waypoint.transform.location) > 4.0:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def save_video(self, episode_num, output_dir=\"eval_videos\"):\n",
    "        \"\"\"Save recorded frames as video\"\"\"\n",
    "        if not self.recording_frames:\n",
    "            print(f\"  No frames to save for episode {episode_num}\")\n",
    "            return None\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        video_path = os.path.join(output_dir, f\"episode_{episode_num}.mp4\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"  Creating video from {len(self.recording_frames)} frames...\")\n",
    "            with imageio.get_writer(video_path, fps=20) as writer:\n",
    "                for frame in self.recording_frames:\n",
    "                    writer.append_data(frame)\n",
    "            print(f\"  ✓ Video saved: {video_path}\")\n",
    "            return video_path\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Video creation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        if self.vehicle:\n",
    "            self.vehicle.destroy()\n",
    "        for sensor in self.sensors:\n",
    "            if sensor.is_alive:\n",
    "                sensor.destroy()\n",
    "        self.sensors.clear()\n",
    "        self.vehicle = None\n",
    "        self.camera = None\n",
    "    \n",
    "    def close(self):\n",
    "        self._cleanup()\n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = False\n",
    "        self.world.apply_settings(settings)\n",
    "\n",
    "def evaluate(model_path, num_episodes=5, max_steps=1000):\n",
    "    print(\"=\"*70)\n",
    "    print(\"CARLA DQN AGENT EVALUATION (SMOOTH & SLOW)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DQN(state_dim=15, action_dim=9).to(device)  # Changed to 15 dims\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "        print(f\"✓ Model loaded: {model_path}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load model: {e}\")\n",
    "        return\n",
    "    \n",
    "    env = CarlaEvalEnv(visualize=True)\n",
    "    \n",
    "    all_distances = []\n",
    "    all_speeds = []\n",
    "    all_collisions = 0\n",
    "    all_lane_invasions = 0\n",
    "    video_paths = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"EPISODE {ep+1}/{num_episodes}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        state = env.reset(spawn_idx=ep)\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        print(\"Running... (Recording video)\")\n",
    "        print(\"Speed: REDUCED for smooth evaluation\\n\")\n",
    "        action_counts = {i: 0 for i in range(9)}\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_tensor)\n",
    "                action = q_values.argmax().item()\n",
    "            \n",
    "            action_counts[action] += 1\n",
    "            next_state, done = env.step(action)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if steps % 50 == 0:\n",
    "                avg_speed = np.mean(env.speeds[-50:])\n",
    "                avg_lane = np.mean(env.lane_offsets[-50:])\n",
    "                print(f\"  Step {steps:4d}: Speed={avg_speed:4.1f} m/s | \"\n",
    "                      f\"Distance={env.distance_traveled:6.1f}m | \"\n",
    "                      f\"Lane offset={avg_lane:4.2f}m\")\n",
    "        \n",
    "        # Episode summary\n",
    "        avg_speed = np.mean(env.speeds) if env.speeds else 0\n",
    "        avg_lane_offset = np.mean(env.lane_offsets) if env.lane_offsets else 0\n",
    "        max_lane_offset = max(env.lane_offsets) if env.lane_offsets else 0\n",
    "        \n",
    "        all_distances.append(env.distance_traveled)\n",
    "        all_speeds.append(avg_speed)\n",
    "        if len(env.collision_hist) > 0:\n",
    "            all_collisions += 1\n",
    "        all_lane_invasions += len(env.lane_invasion_hist)\n",
    "        \n",
    "        print(f\"\\n{'─'*70}\")\n",
    "        print(f\"EPISODE {ep+1} SUMMARY\")\n",
    "        print(f\"{'─'*70}\")\n",
    "        print(f\"Outcome: {'✗ COLLISION' if len(env.collision_hist) > 0 else '✓ SUCCESS'}\")\n",
    "        print(f\"\\nDriving Performance:\")\n",
    "        print(f\"  • Distance Traveled: {env.distance_traveled:.1f} m\")\n",
    "        print(f\"  • Average Speed: {avg_speed:.1f} m/s ({avg_speed*3.6:.1f} km/h)\")\n",
    "        print(f\"  • Episode Duration: {steps} steps ({steps*0.05:.1f} seconds)\")\n",
    "        print(f\"\\nLane Keeping:\")\n",
    "        print(f\"  • Average Lane Offset: {avg_lane_offset:.3f} m\")\n",
    "        print(f\"  • Maximum Lane Offset: {max_lane_offset:.3f} m\")\n",
    "        print(f\"  • Lane Invasions: {len(env.lane_invasion_hist)}\")\n",
    "        print(f\"\\nSafety:\")\n",
    "        print(f\"  • Collisions: {len(env.collision_hist)}\")\n",
    "        \n",
    "        # Save video for this episode\n",
    "        video_path = env.save_video(ep + 1)\n",
    "        if video_path:\n",
    "            video_paths.append(video_path)\n",
    "        \n",
    "        print()\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Overall summary\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"OVERALL EVALUATION RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    success_rate = (num_episodes - all_collisions) / num_episodes\n",
    "    avg_distance = np.mean(all_distances)\n",
    "    avg_speed = np.mean(all_speeds)\n",
    "    \n",
    "    print(f\"\\nSuccess Rate: {success_rate*100:.1f}% ({num_episodes - all_collisions}/{num_episodes} episodes)\")\n",
    "    print(f\"Average Distance: {avg_distance:.1f} m\")\n",
    "    print(f\"Average Speed: {avg_speed:.1f} m/s ({avg_speed*3.6:.1f} km/h)\")\n",
    "    print(f\"Total Collisions: {all_collisions}\")\n",
    "    print(f\"Total Lane Invasions: {all_lane_invasions}\")\n",
    "    \n",
    "    # Video summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"VIDEO RECORDINGS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Videos saved: {len(video_paths)}/{num_episodes}\")\n",
    "    for vp in video_paths:\n",
    "        print(f\"  • {vp}\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PERFORMANCE ASSESSMENT\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Adjusted scoring for slower speeds\n",
    "    speed_score = min(100, (avg_speed / 7.0) * 100)  # Target 7 m/s instead of 10\n",
    "    safety_score = success_rate * 100\n",
    "    distance_score = min(100, (avg_distance / 500.0) * 100)\n",
    "    overall_score = (speed_score + safety_score + distance_score) / 3\n",
    "    \n",
    "    print(f\"\\nSpeed Score:    {speed_score:5.1f}% (target: 7+ m/s for smooth driving)\")\n",
    "    print(f\"Safety Score:   {safety_score:5.1f}% (no collisions)\")\n",
    "    print(f\"Distance Score: {distance_score:5.1f}% (target: 500+ m)\")\n",
    "    print(f\"Overall Score:  {overall_score:5.1f}%\")\n",
    "    \n",
    "    if overall_score >= 80:\n",
    "        grade = \"A - Excellent smooth autonomous driving!\"\n",
    "    elif overall_score >= 70:\n",
    "        grade = \"B - Good, minor improvements needed\"\n",
    "    elif overall_score >= 60:\n",
    "        grade = \"C - Fair, needs work on consistency\"\n",
    "    elif overall_score >= 50:\n",
    "        grade = \"D - Poor, basic driving but unsafe\"\n",
    "    else:\n",
    "        grade = \"F - Failed to demonstrate autonomous driving\"\n",
    "    \n",
    "    print(f\"\\nFINAL GRADE: {grade}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', default='carla_dqn_agent_final.pth')\n",
    "    parser.add_argument('--episodes', type=int, default=5)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"\\nMake sure CARLA is running!\\n\")\n",
    "    input(\"Press Enter to start...\")\n",
    "    \n",
    "    evaluate(args.model, args.episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CARLA PPO Training - Optimized for Lane Keeping + Turns\n",
    "Complete solution with proper reward shaping and real-time plotting\n",
    "\n",
    "Task Requirements:\n",
    "1. Maintain lane center while driving\n",
    "2. Detect and follow lane change arrows\n",
    "3. Execute smooth turns\n",
    "4. Return to lane center after turns\n",
    "\n",
    "Model: PPO (better than DQN for continuous control)\n",
    "\"\"\"\n",
    "\n",
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "EPISODES = 500\n",
    "MAX_STEPS = 800\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "LEARNING_RATE = 3e-4\n",
    "ENTROPY_COEF = 0.01\n",
    "VALUE_COEF = 0.5\n",
    "UPDATE_EPOCHS = 10\n",
    "TARGET_SPEED = 7.0  # m/s\n",
    "\n",
    "# ==================== PPO ACTOR-CRITIC ====================\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim=12, action_dim=2):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor head (policy) - outputs mean and log_std for continuous actions\n",
    "        self.actor_mean = nn.Linear(256, action_dim)\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "        # Critic head (value function)\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "        \n",
    "        # Special init for policy head (smaller initial actions)\n",
    "        nn.init.orthogonal_(self.actor_mean.weight, gain=0.01)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        features = self.shared(state)\n",
    "        return features\n",
    "    \n",
    "    def act(self, state):\n",
    "        features = self.forward(state)\n",
    "        \n",
    "        # Policy\n",
    "        action_mean = self.actor_mean(features)\n",
    "        action_std = torch.exp(self.actor_log_std)\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        action = dist.sample()\n",
    "        action_log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        \n",
    "        # Value\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action, action_log_prob, value\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        features = self.forward(state)\n",
    "        \n",
    "        # Policy\n",
    "        action_mean = self.actor_mean(features)\n",
    "        action_std = torch.exp(self.actor_log_std)\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        action_log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        \n",
    "        # Value\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action_log_prob, entropy, value\n",
    "\n",
    "\n",
    "# ==================== PPO MEMORY ====================\n",
    "class PPOMemory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def store(self, state, action, log_prob, reward, value, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def get(self):\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(self.states)),\n",
    "            torch.FloatTensor(np.array(self.actions)),\n",
    "            torch.FloatTensor(np.array(self.log_probs)),\n",
    "            torch.FloatTensor(np.array(self.rewards)),\n",
    "            torch.FloatTensor(np.array(self.values)),\n",
    "            torch.FloatTensor(np.array(self.dones))\n",
    "        )\n",
    "\n",
    "\n",
    "# ==================== CARLA ENVIRONMENT ====================\n",
    "class CarlaLaneKeepingEnv:\n",
    "    def __init__(self):\n",
    "        self.client = carla.Client('localhost', 2000)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.load_world('Town04')\n",
    "        \n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = True\n",
    "        settings.fixed_delta_seconds = 0.05\n",
    "        self.world.apply_settings(settings)\n",
    "        \n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.map = self.world.get_map()\n",
    "        self.vehicle = None\n",
    "        self.sensors = []\n",
    "        \n",
    "        self.collision_hist = []\n",
    "        self.spawn_points = self.map.get_spawn_points()\n",
    "        \n",
    "        self.distance_traveled = 0.0\n",
    "        self.last_location = None\n",
    "        self.steps = 0\n",
    "        \n",
    "        # For reward calculation\n",
    "        self.last_lane_offset = 0.0\n",
    "        self.last_heading_error = 0.0\n",
    "    \n",
    "    def reset(self):\n",
    "        self._cleanup()\n",
    "        \n",
    "        vehicle_bp = self.blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "        spawn_point = np.random.choice(self.spawn_points[:30])\n",
    "        \n",
    "        try:\n",
    "            self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        except:\n",
    "            spawn_point = np.random.choice(self.spawn_points)\n",
    "            self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        \n",
    "        # Collision sensor\n",
    "        collision_bp = self.blueprint_library.find('sensor.other.collision')\n",
    "        collision_sensor = self.world.spawn_actor(\n",
    "            collision_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        collision_sensor.listen(lambda e: self.collision_hist.append(e))\n",
    "        self.sensors.append(collision_sensor)\n",
    "        \n",
    "        # Reset tracking\n",
    "        self.collision_hist.clear()\n",
    "        self.last_location = self.vehicle.get_location()\n",
    "        self.distance_traveled = 0.0\n",
    "        self.steps = 0\n",
    "        \n",
    "        self.last_lane_offset = 0.0\n",
    "        self.last_heading_error = 0.0\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            self.world.tick()\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"12-dimensional state vector optimized for lane keeping\"\"\"\n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        waypoint = self.map.get_waypoint(\n",
    "            transform.location, project_to_road=True, lane_type=carla.LaneType.Driving\n",
    "        )\n",
    "        \n",
    "        if not waypoint:\n",
    "            return np.zeros(12, dtype=np.float32)\n",
    "        \n",
    "        # 1. Lane offset (most important!)\n",
    "        lane_center = waypoint.transform.location\n",
    "        offset_vec = transform.location - lane_center\n",
    "        right_vec = transform.get_right_vector()\n",
    "        lane_offset = offset_vec.x * right_vec.x + offset_vec.y * right_vec.y\n",
    "        \n",
    "        # 2. Heading error\n",
    "        lane_yaw = waypoint.transform.rotation.yaw\n",
    "        vehicle_yaw = transform.rotation.yaw\n",
    "        heading_error = (vehicle_yaw - lane_yaw + 180) % 360 - 180\n",
    "        heading_error_rad = np.radians(heading_error)\n",
    "        \n",
    "        # 3. Look-ahead waypoint\n",
    "        next_wps = waypoint.next(10.0)\n",
    "        if next_wps:\n",
    "            next_wp = next_wps[0]\n",
    "            wp_vec = next_wp.transform.location - transform.location\n",
    "            forward_vec = transform.get_forward_vector()\n",
    "            angle_to_wp = np.arctan2(\n",
    "                wp_vec.y * forward_vec.x - wp_vec.x * forward_vec.y,\n",
    "                wp_vec.x * forward_vec.x + wp_vec.y * forward_vec.y\n",
    "            )\n",
    "        else:\n",
    "            angle_to_wp = 0.0\n",
    "        \n",
    "        # 4. Curvature\n",
    "        curvature = 0.0\n",
    "        if next_wps:\n",
    "            curvature_yaw = next_wps[0].transform.rotation.yaw - lane_yaw\n",
    "            curvature = np.sin(np.radians(curvature_yaw))\n",
    "        \n",
    "        # 5. Speed normalized\n",
    "        speed_norm = speed / 15.0\n",
    "        \n",
    "        # 6. Velocity components\n",
    "        vel_x = velocity.x / 15.0\n",
    "        vel_y = velocity.y / 15.0\n",
    "        \n",
    "        # 7. Junction detection\n",
    "        is_junction = 1.0 if waypoint.is_junction else 0.0\n",
    "        \n",
    "        state = np.array([\n",
    "            lane_offset / 3.0,           # 0: Lane offset (normalized)\n",
    "            np.sin(heading_error_rad),   # 1: Heading error sin\n",
    "            np.cos(heading_error_rad),   # 2: Heading error cos\n",
    "            np.sin(angle_to_wp),         # 3: Angle to waypoint sin\n",
    "            np.cos(angle_to_wp),         # 4: Angle to waypoint cos\n",
    "            curvature,                   # 5: Road curvature\n",
    "            speed_norm,                  # 6: Speed\n",
    "            vel_x,                       # 7: Velocity X\n",
    "            vel_y,                       # 8: Velocity Y\n",
    "            is_junction,                 # 9: Junction flag\n",
    "            lane_offset / 3.0 - self.last_lane_offset / 3.0,  # 10: Lane offset change\n",
    "            heading_error_rad - self.last_heading_error,      # 11: Heading change\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        self.last_lane_offset = lane_offset\n",
    "        self.last_heading_error = heading_error_rad\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Action: [steering, throttle_delta]\n",
    "        steering: [-1, 1]\n",
    "        throttle_delta: [-1, 1] (added to base throttle)\n",
    "        \"\"\"\n",
    "        steering = float(np.clip(action[0], -0.4, 0.4))  # Limit max steering\n",
    "        \n",
    "        # Base throttle for target speed, action modulates it\n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        # Simple speed controller\n",
    "        speed_error = TARGET_SPEED - speed\n",
    "        base_throttle = 0.5 + 0.1 * speed_error\n",
    "        throttle_delta = float(np.clip(action[1], -0.3, 0.3))\n",
    "        throttle = float(np.clip(base_throttle + throttle_delta, 0.0, 0.8))\n",
    "        \n",
    "        # Apply control\n",
    "        control = carla.VehicleControl(\n",
    "            throttle=throttle,\n",
    "            steer=steering,\n",
    "            brake=0.0\n",
    "        )\n",
    "        \n",
    "        self.vehicle.apply_control(control)\n",
    "        self.world.tick()\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Update distance\n",
    "        current_location = self.vehicle.get_location()\n",
    "        if self.last_location:\n",
    "            self.distance_traveled += current_location.distance(self.last_location)\n",
    "        self.last_location = current_location\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward(next_state, steering, throttle)\n",
    "        \n",
    "        # Check done\n",
    "        done = self._is_done()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def _calculate_reward(self, state, steering, throttle):\n",
    "        \"\"\"Reward function optimized for lane keeping\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Extract state components\n",
    "        lane_offset = state[0] * 3.0  # Denormalize\n",
    "        heading_sin = state[1]\n",
    "        heading_cos = state[2]\n",
    "        heading_error = np.arctan2(heading_sin, heading_cos)\n",
    "        curvature = state[5]\n",
    "        speed = state[6] * 15.0\n",
    "        \n",
    "        # 1. LANE CENTERING (most important!) - Exponential reward\n",
    "        lane_reward = 5.0 * np.exp(-3.0 * abs(lane_offset))\n",
    "        reward += lane_reward\n",
    "        \n",
    "        # 2. HEADING ALIGNMENT - Stay parallel to lane\n",
    "        heading_reward = 3.0 * np.exp(-2.0 * abs(heading_error))\n",
    "        reward += heading_reward\n",
    "        \n",
    "        # 3. FORWARD PROGRESS - Reward distance traveled\n",
    "        distance_reward = 0.1  # Small constant reward for each step\n",
    "        reward += distance_reward\n",
    "        \n",
    "        # 4. SPEED MAINTENANCE - Stay near target speed\n",
    "        speed_error = abs(speed - TARGET_SPEED)\n",
    "        speed_reward = 2.0 * np.exp(-speed_error / 3.0)\n",
    "        reward += speed_reward\n",
    "        \n",
    "        # 5. SMOOTH STEERING - Penalize jerky movements\n",
    "        steering_penalty = -0.5 * abs(steering)\n",
    "        reward += steering_penalty\n",
    "        \n",
    "        # 6. PENALTIES\n",
    "        # Severe off-center penalty\n",
    "        if abs(lane_offset) > 1.0:\n",
    "            reward -= 5.0\n",
    "        \n",
    "        # Collision penalty\n",
    "        if len(self.collision_hist) > 0:\n",
    "            reward -= 100.0\n",
    "        \n",
    "        # Off-road penalty\n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        if waypoint is None or transform.location.distance(waypoint.transform.location) > 3.5:\n",
    "            reward -= 50.0\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _is_done(self):\n",
    "        # Collision\n",
    "        if len(self.collision_hist) > 0:\n",
    "            return True\n",
    "        \n",
    "        # Off road\n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        if waypoint is None:\n",
    "            return True\n",
    "        if transform.location.distance(waypoint.transform.location) > 3.5:\n",
    "            return True\n",
    "        \n",
    "        # Max steps\n",
    "        if self.steps >= MAX_STEPS:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        if self.vehicle:\n",
    "            self.vehicle.destroy()\n",
    "        for sensor in self.sensors:\n",
    "            if sensor.is_alive:\n",
    "                sensor.destroy()\n",
    "        self.sensors.clear()\n",
    "        self.vehicle = None\n",
    "    \n",
    "    def close(self):\n",
    "        self._cleanup()\n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = False\n",
    "        self.world.apply_settings(settings)\n",
    "\n",
    "\n",
    "# ==================== PPO AGENT ====================\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim=12, action_dim=2):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        self.memory = PPOMemory()\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, value = self.policy.act(state_tensor)\n",
    "        \n",
    "        return action.cpu().numpy()[0], log_prob.cpu().item(), value.cpu().item()\n",
    "    \n",
    "    def update(self):\n",
    "        # Get data from memory\n",
    "        states, actions, old_log_probs, rewards, values, dones = self.memory.get()\n",
    "        \n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        old_log_probs = old_log_probs.to(self.device)\n",
    "        \n",
    "        # Compute advantages using GAE\n",
    "        advantages = []\n",
    "        returns = []\n",
    "        \n",
    "        gae = 0\n",
    "        next_value = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = 0\n",
    "                next_done = 0\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "                next_done = dones[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + GAMMA * next_value * (1 - next_done) - values[t]\n",
    "            gae = delta + GAMMA * GAE_LAMBDA * (1 - next_done) * gae\n",
    "            \n",
    "            advantages.insert(0, gae)\n",
    "            returns.insert(0, gae + values[t])\n",
    "        \n",
    "        advantages = torch.FloatTensor(advantages).to(self.device)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO update\n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy = 0\n",
    "        \n",
    "        for _ in range(UPDATE_EPOCHS):\n",
    "            # Evaluate\n",
    "            log_probs, entropy, state_values = self.policy.evaluate(states, actions)\n",
    "            \n",
    "            # Ratio\n",
    "            ratios = torch.exp(log_probs - old_log_probs)\n",
    "            \n",
    "            # Surrogate losses\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * advantages\n",
    "            \n",
    "            # Final loss\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = 0.5 * ((returns - state_values.squeeze()) ** 2).mean()\n",
    "            entropy_loss = -entropy.mean()\n",
    "            \n",
    "            loss = policy_loss + VALUE_COEF * value_loss + ENTROPY_COEF * entropy_loss\n",
    "            \n",
    "            # Backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_policy_loss += policy_loss.item()\n",
    "            total_value_loss += value_loss.item()\n",
    "            total_entropy += entropy_loss.item()\n",
    "        \n",
    "        # Clear memory\n",
    "        self.memory.clear()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': total_policy_loss / UPDATE_EPOCHS,\n",
    "            'value_loss': total_value_loss / UPDATE_EPOCHS,\n",
    "            'entropy': -total_entropy / UPDATE_EPOCHS\n",
    "        }\n",
    "\n",
    "\n",
    "# ==================== TRAINING ====================\n",
    "def train():\n",
    "    print(\"=\"*80)\n",
    "    print(\"PPO TRAINING - LANE KEEPING + TURNS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Episodes: {EPISODES}\")\n",
    "    print(f\"Max steps per episode: {MAX_STEPS}\")\n",
    "    print(f\"Target speed: {TARGET_SPEED} m/s\\n\")\n",
    "    \n",
    "    env = CarlaLaneKeepingEnv()\n",
    "    agent = PPOAgent(state_dim=12, action_dim=2)\n",
    "    \n",
    "    # Tracking\n",
    "    episode_rewards = []\n",
    "    episode_distances = []\n",
    "    episode_lengths = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    \n",
    "    # Setup plotting\n",
    "    plt.ion()\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    best_distance = 0.0\n",
    "    \n",
    "    print(\"Starting training...\\n\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for episode in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action, log_prob, value = agent.select_action(state)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Store in memory\n",
    "            agent.memory.store(state, action, log_prob, reward, value, done)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        # Update policy\n",
    "        losses = agent.update()\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_distances.append(env.distance_traveled)\n",
    "        episode_lengths.append(env.steps)\n",
    "        policy_losses.append(losses['policy_loss'])\n",
    "        value_losses.append(losses['value_loss'])\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_distance = np.mean(episode_distances[-10:])\n",
    "            avg_length = np.mean(episode_lengths[-10:])\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            \n",
    "            print(f\"Ep {episode+1:3d} | \"\n",
    "                  f\"R: {episode_reward:7.1f} | \"\n",
    "                  f\"D: {env.distance_traveled:6.1f}m | \"\n",
    "                  f\"L: {env.steps:3d} | \"\n",
    "                  f\"Avg10: R={avg_reward:6.1f}, D={avg_distance:5.1f}m | \"\n",
    "                  f\"Loss: P={losses['policy_loss']:.3f}, V={losses['value_loss']:.3f} | \"\n",
    "                  f\"T: {elapsed:.1f}min\")\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_distance > best_distance:\n",
    "                best_distance = avg_distance\n",
    "                torch.save(agent.policy.state_dict(), 'ppo_lane_keeping_best.pth')\n",
    "                print(f\"  ✓ New best! Avg distance: {avg_distance:.1f}m\")\n",
    "        \n",
    "        # Update plots\n",
    "        if (episode + 1) % 5 == 0:\n",
    "            ax1.clear()\n",
    "            ax1.plot(episode_rewards, alpha=0.3)\n",
    "            ax1.plot(np.convolve(episode_rewards, np.ones(10)/10, mode='valid'))\n",
    "            ax1.set_title('Episode Rewards')\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Reward')\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            ax2.clear()\n",
    "            ax2.plot(episode_distances, alpha=0.3)\n",
    "            ax2.plot(np.convolve(episode_distances, np.ones(10)/10, mode='valid'))\n",
    "            ax2.set_title('Distance Traveled')\n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Distance (m)')\n",
    "            ax2.grid(True)\n",
    "            \n",
    "            ax3.clear()\n",
    "            ax3.plot(policy_losses, label='Policy Loss')\n",
    "            ax3.plot(value_losses, label='Value Loss')\n",
    "            ax3.set_title('Training Losses')\n",
    "            ax3.set_xlabel('Episode')\n",
    "            ax3.set_ylabel('Loss')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True)\n",
    "            \n",
    "            ax4.clear()\n",
    "            ax4.plot(episode_lengths, alpha=0.3)\n",
    "            ax4.plot(np.convolve(episode_lengths, np.ones(10)/10, mode='valid'))\n",
    "            ax4.set_title('Episode Length')\n",
    "            ax4.set_xlabel('Episode')\n",
    "            ax4.set_ylabel('Steps')\n",
    "            ax4.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.pause(0.01)\n",
    "            \n",
    "            # Save plot\n",
    "            plt.savefig('ppo_training_progress.png', dpi=150)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            torch.save(agent.policy.state_dict(), f'ppo_checkpoint_ep{episode+1}.pth')\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(agent.policy.state_dict(), 'ppo_lane_keeping_final.pth')\n",
    "    \n",
    "    print(f\"\\n✓ Training completed in {(time.time() - start_time) / 60:.1f} minutes\")\n",
    "    print(f\"✓ Best avg distance: {best_distance:.1f}m\")\n",
    "    print(f\"✓ Models saved\")\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for training: /home/robotuser/carla_0.9.12/PythonAPI/examples/PPO_best_git/ppo_lane_keeping_final.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Continue PPO Training from Checkpoint\n",
    "Loads your existing trained model and trains for 500 more episodes\n",
    "\n",
    "Expected improvements after 500 more episodes:\n",
    "- Distance: 923m → 1100-1300m\n",
    "- Lane offset: 0.175m → 0.12-0.15m  \n",
    "- Speed: 5.0 m/s → 6.0-6.5 m/s\n",
    "\"\"\"\n",
    "\n",
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "EPISODES = 500  # Train 500 MORE episodes\n",
    "MAX_STEPS = 800\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "LEARNING_RATE = 3e-4\n",
    "ENTROPY_COEF = 0.01\n",
    "VALUE_COEF = 0.5\n",
    "UPDATE_EPOCHS = 10\n",
    "TARGET_SPEED = 7.0\n",
    "\n",
    "# CHECKPOINT TO LOAD\n",
    "CHECKPOINT_PATH = 'ppo_lane_keeping_best.pth'\n",
    "START_EPISODE = 500  # Your training ended at episode 500\n",
    "\n",
    "# ==================== COPY YOUR EXISTING CLASSES ====================\n",
    "# (ActorCritic, PPOMemory, CarlaLaneKeepingEnv, PPOAgent)\n",
    "# Exactly the same as your training script\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim=12, action_dim=2):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.actor_mean = nn.Linear(256, action_dim)\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "        nn.init.orthogonal_(self.actor_mean.weight, gain=0.01)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.shared(state)\n",
    "    \n",
    "    def act(self, state):\n",
    "        features = self.forward(state)\n",
    "        action_mean = self.actor_mean(features)\n",
    "        action_std = torch.exp(self.actor_log_std)\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        action = dist.sample()\n",
    "        action_log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        value = self.critic(features)\n",
    "        return action, action_log_prob, value\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        features = self.forward(state)\n",
    "        action_mean = self.actor_mean(features)\n",
    "        action_std = torch.exp(self.actor_log_std)\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        action_log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        value = self.critic(features)\n",
    "        return action_log_prob, entropy, value\n",
    "\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def store(self, state, action, log_prob, reward, value, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def get(self):\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(self.states)),\n",
    "            torch.FloatTensor(np.array(self.actions)),\n",
    "            torch.FloatTensor(np.array(self.log_probs)),\n",
    "            torch.FloatTensor(np.array(self.rewards)),\n",
    "            torch.FloatTensor(np.array(self.values)),\n",
    "            torch.FloatTensor(np.array(self.dones))\n",
    "        )\n",
    "\n",
    "\n",
    "class CarlaLaneKeepingEnv:\n",
    "    def __init__(self):\n",
    "        self.client = carla.Client('localhost', 2000)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.load_world('Town04')\n",
    "        \n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = True\n",
    "        settings.fixed_delta_seconds = 0.05\n",
    "        self.world.apply_settings(settings)\n",
    "        \n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.map = self.world.get_map()\n",
    "        self.vehicle = None\n",
    "        self.sensors = []\n",
    "        self.collision_hist = []\n",
    "        self.spawn_points = self.map.get_spawn_points()\n",
    "        \n",
    "        self.distance_traveled = 0.0\n",
    "        self.last_location = None\n",
    "        self.steps = 0\n",
    "        self.last_lane_offset = 0.0\n",
    "        self.last_heading_error = 0.0\n",
    "    \n",
    "    def reset(self):\n",
    "        self._cleanup()\n",
    "        \n",
    "        vehicle_bp = self.blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "        spawn_point = np.random.choice(self.spawn_points[:30])\n",
    "        \n",
    "        try:\n",
    "            self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        except:\n",
    "            spawn_point = np.random.choice(self.spawn_points)\n",
    "            self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        \n",
    "        collision_bp = self.blueprint_library.find('sensor.other.collision')\n",
    "        collision_sensor = self.world.spawn_actor(\n",
    "            collision_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        collision_sensor.listen(lambda e: self.collision_hist.append(e))\n",
    "        self.sensors.append(collision_sensor)\n",
    "        \n",
    "        self.collision_hist.clear()\n",
    "        self.last_location = self.vehicle.get_location()\n",
    "        self.distance_traveled = 0.0\n",
    "        self.steps = 0\n",
    "        self.last_lane_offset = 0.0\n",
    "        self.last_heading_error = 0.0\n",
    "        \n",
    "        for _ in range(10):\n",
    "            self.world.tick()\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        waypoint = self.map.get_waypoint(\n",
    "            transform.location, project_to_road=True, lane_type=carla.LaneType.Driving\n",
    "        )\n",
    "        \n",
    "        if not waypoint:\n",
    "            return np.zeros(12, dtype=np.float32)\n",
    "        \n",
    "        lane_center = waypoint.transform.location\n",
    "        offset_vec = transform.location - lane_center\n",
    "        right_vec = transform.get_right_vector()\n",
    "        lane_offset = offset_vec.x * right_vec.x + offset_vec.y * right_vec.y\n",
    "        \n",
    "        lane_yaw = waypoint.transform.rotation.yaw\n",
    "        vehicle_yaw = transform.rotation.yaw\n",
    "        heading_error = (vehicle_yaw - lane_yaw + 180) % 360 - 180\n",
    "        heading_error_rad = np.radians(heading_error)\n",
    "        \n",
    "        next_wps = waypoint.next(10.0)\n",
    "        if next_wps:\n",
    "            next_wp = next_wps[0]\n",
    "            wp_vec = next_wp.transform.location - transform.location\n",
    "            forward_vec = transform.get_forward_vector()\n",
    "            angle_to_wp = np.arctan2(\n",
    "                wp_vec.y * forward_vec.x - wp_vec.x * forward_vec.y,\n",
    "                wp_vec.x * forward_vec.x + wp_vec.y * forward_vec.y\n",
    "            )\n",
    "        else:\n",
    "            angle_to_wp = 0.0\n",
    "        \n",
    "        curvature = 0.0\n",
    "        if next_wps:\n",
    "            curvature_yaw = next_wps[0].transform.rotation.yaw - lane_yaw\n",
    "            curvature = np.sin(np.radians(curvature_yaw))\n",
    "        \n",
    "        speed_norm = speed / 15.0\n",
    "        vel_x = velocity.x / 15.0\n",
    "        vel_y = velocity.y / 15.0\n",
    "        is_junction = 1.0 if waypoint.is_junction else 0.0\n",
    "        \n",
    "        state = np.array([\n",
    "            lane_offset / 3.0,\n",
    "            np.sin(heading_error_rad),\n",
    "            np.cos(heading_error_rad),\n",
    "            np.sin(angle_to_wp),\n",
    "            np.cos(angle_to_wp),\n",
    "            curvature,\n",
    "            speed_norm,\n",
    "            vel_x,\n",
    "            vel_y,\n",
    "            is_junction,\n",
    "            lane_offset / 3.0 - self.last_lane_offset / 3.0,\n",
    "            heading_error_rad - self.last_heading_error,\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        self.last_lane_offset = lane_offset\n",
    "        self.last_heading_error = heading_error_rad\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        steering = float(np.clip(action[0], -0.4, 0.4))\n",
    "        \n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        speed_error = TARGET_SPEED - speed\n",
    "        base_throttle = 0.5 + 0.1 * speed_error\n",
    "        throttle_delta = float(np.clip(action[1], -0.3, 0.3))\n",
    "        throttle = float(np.clip(base_throttle + throttle_delta, 0.0, 0.8))\n",
    "        \n",
    "        control = carla.VehicleControl(\n",
    "            throttle=throttle,\n",
    "            steer=steering,\n",
    "            brake=0.0\n",
    "        )\n",
    "        \n",
    "        self.vehicle.apply_control(control)\n",
    "        self.world.tick()\n",
    "        self.steps += 1\n",
    "        \n",
    "        current_location = self.vehicle.get_location()\n",
    "        if self.last_location:\n",
    "            self.distance_traveled += current_location.distance(self.last_location)\n",
    "        self.last_location = current_location\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        reward = self._calculate_reward(next_state, steering, throttle)\n",
    "        done = self._is_done()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def _calculate_reward(self, state, steering, throttle):\n",
    "        reward = 0.0\n",
    "        \n",
    "        lane_offset = state[0] * 3.0\n",
    "        heading_sin = state[1]\n",
    "        heading_cos = state[2]\n",
    "        heading_error = np.arctan2(heading_sin, heading_cos)\n",
    "        speed = state[6] * 15.0\n",
    "        \n",
    "        lane_reward = 5.0 * np.exp(-3.0 * abs(lane_offset))\n",
    "        reward += lane_reward\n",
    "        \n",
    "        heading_reward = 3.0 * np.exp(-2.0 * abs(heading_error))\n",
    "        reward += heading_reward\n",
    "        \n",
    "        reward += 0.1\n",
    "        \n",
    "        speed_error = abs(speed - TARGET_SPEED)\n",
    "        speed_reward = 2.0 * np.exp(-speed_error / 3.0)\n",
    "        reward += speed_reward\n",
    "        \n",
    "        steering_penalty = -0.5 * abs(steering)\n",
    "        reward += steering_penalty\n",
    "        \n",
    "        if abs(lane_offset) > 1.0:\n",
    "            reward -= 5.0\n",
    "        \n",
    "        if len(self.collision_hist) > 0:\n",
    "            reward -= 100.0\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        if waypoint is None or transform.location.distance(waypoint.transform.location) > 3.5:\n",
    "            reward -= 50.0\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _is_done(self):\n",
    "        if len(self.collision_hist) > 0:\n",
    "            return True\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        if waypoint is None:\n",
    "            return True\n",
    "        if transform.location.distance(waypoint.transform.location) > 3.5:\n",
    "            return True\n",
    "        \n",
    "        if self.steps >= MAX_STEPS:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        if self.vehicle:\n",
    "            self.vehicle.destroy()\n",
    "        for sensor in self.sensors:\n",
    "            if sensor.is_alive:\n",
    "                sensor.destroy()\n",
    "        self.sensors.clear()\n",
    "        self.vehicle = None\n",
    "    \n",
    "    def close(self):\n",
    "        self._cleanup()\n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = False\n",
    "        self.world.apply_settings(settings)\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim=12, action_dim=2):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = PPOMemory()\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, value = self.policy.act(state_tensor)\n",
    "        \n",
    "        return action.cpu().numpy()[0], log_prob.cpu().item(), value.cpu().item()\n",
    "    \n",
    "    def update(self):\n",
    "        states, actions, old_log_probs, rewards, values, dones = self.memory.get()\n",
    "        \n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        old_log_probs = old_log_probs.to(self.device)\n",
    "        \n",
    "        advantages = []\n",
    "        returns = []\n",
    "        \n",
    "        gae = 0\n",
    "        next_value = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = 0\n",
    "                next_done = 0\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "                next_done = dones[t + 1]\n",
    "            \n",
    "            delta = rewards[t] + GAMMA * next_value * (1 - next_done) - values[t]\n",
    "            gae = delta + GAMMA * GAE_LAMBDA * (1 - next_done) * gae\n",
    "            \n",
    "            advantages.insert(0, gae)\n",
    "            returns.insert(0, gae + values[t])\n",
    "        \n",
    "        advantages = torch.FloatTensor(advantages).to(self.device)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy = 0\n",
    "        \n",
    "        for _ in range(UPDATE_EPOCHS):\n",
    "            log_probs, entropy, state_values = self.policy.evaluate(states, actions)\n",
    "            \n",
    "            ratios = torch.exp(log_probs - old_log_probs)\n",
    "            \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * advantages\n",
    "            \n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = 0.5 * ((returns - state_values.squeeze()) ** 2).mean()\n",
    "            entropy_loss = -entropy.mean()\n",
    "            \n",
    "            loss = policy_loss + VALUE_COEF * value_loss + ENTROPY_COEF * entropy_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_policy_loss += policy_loss.item()\n",
    "            total_value_loss += value_loss.item()\n",
    "            total_entropy += entropy_loss.item()\n",
    "        \n",
    "        self.memory.clear()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': total_policy_loss / UPDATE_EPOCHS,\n",
    "            'value_loss': total_value_loss / UPDATE_EPOCHS,\n",
    "            'entropy': -total_entropy / UPDATE_EPOCHS\n",
    "        }\n",
    "\n",
    "\n",
    "# ==================== CONTINUE TRAINING ====================\n",
    "def continue_training():\n",
    "    print(\"=\"*80)\n",
    "    print(\"CONTINUING PPO TRAINING FROM CHECKPOINT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "    print(f\"Additional episodes: {EPISODES}\")\n",
    "    print(f\"Starting from episode: {START_EPISODE}\\n\")\n",
    "    \n",
    "    env = CarlaLaneKeepingEnv()\n",
    "    agent = PPOAgent(state_dim=12, action_dim=2)\n",
    "    \n",
    "    # LOAD CHECKPOINT\n",
    "    try:\n",
    "        agent.policy.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "        print(f\"✓ Checkpoint loaded successfully!\")\n",
    "        print(f\"✓ Continuing training from episode {START_EPISODE}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load checkpoint: {e}\")\n",
    "        return\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_distances = []\n",
    "    episode_lengths = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    \n",
    "    plt.ion()\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    best_distance = 0.0\n",
    "    \n",
    "    print(\"Resuming training...\\n\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for episode in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, log_prob, value = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.memory.store(state, action, log_prob, reward, value, done)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        losses = agent.update()\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_distances.append(env.distance_traveled)\n",
    "        episode_lengths.append(env.steps)\n",
    "        policy_losses.append(losses['policy_loss'])\n",
    "        value_losses.append(losses['value_loss'])\n",
    "        \n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            avg_distance = np.mean(episode_distances[-10:])\n",
    "            avg_length = np.mean(episode_lengths[-10:])\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            \n",
    "            total_ep = START_EPISODE + episode + 1\n",
    "            \n",
    "            print(f\"Ep {total_ep:3d} | \"\n",
    "                  f\"R: {episode_reward:7.1f} | \"\n",
    "                  f\"D: {env.distance_traveled:6.1f}m | \"\n",
    "                  f\"L: {env.steps:3d} | \"\n",
    "                  f\"Avg10: R={avg_reward:6.1f}, D={avg_distance:5.1f}m | \"\n",
    "                  f\"Loss: P={losses['policy_loss']:.3f}, V={losses['value_loss']:.3f} | \"\n",
    "                  f\"T: {elapsed:.1f}min\")\n",
    "            \n",
    "            if avg_distance > best_distance:\n",
    "                best_distance = avg_distance\n",
    "                torch.save(agent.policy.state_dict(), 'ppo_lane_keeping_best_continued.pth')\n",
    "                print(f\"  ✓ New best! Avg distance: {avg_distance:.1f}m\")\n",
    "        \n",
    "        if (episode + 1) % 5 == 0:\n",
    "            ax1.clear()\n",
    "            ax1.plot(episode_rewards, alpha=0.3)\n",
    "            ax1.plot(np.convolve(episode_rewards, np.ones(10)/10, mode='valid'))\n",
    "            ax1.set_title('Episode Rewards (Continued Training)')\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Reward')\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            ax2.clear()\n",
    "            ax2.plot(episode_distances, alpha=0.3)\n",
    "            ax2.plot(np.convolve(episode_distances, np.ones(10)/10, mode='valid'))\n",
    "            ax2.set_title('Distance Traveled (Continued Training)')\n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Distance (m)')\n",
    "            ax2.grid(True)\n",
    "            \n",
    "            ax3.clear()\n",
    "            ax3.plot(policy_losses, label='Policy Loss')\n",
    "            ax3.plot(value_losses, label='Value Loss')\n",
    "            ax3.set_title('Training Losses')\n",
    "            ax3.set_xlabel('Episode')\n",
    "            ax3.set_ylabel('Loss')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True)\n",
    "            \n",
    "            ax4.clear()\n",
    "            ax4.plot(episode_lengths, alpha=0.3)\n",
    "            ax4.plot(np.convolve(episode_lengths, np.ones(10)/10, mode='valid'))\n",
    "            ax4.set_title('Episode Length')\n",
    "            ax4.set_xlabel('Episode')\n",
    "            ax4.set_ylabel('Steps')\n",
    "            ax4.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.pause(0.01)\n",
    "            plt.savefig(f'ppo_continued_training_ep{START_EPISODE + episode + 1}.png', dpi=150)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            torch.save(agent.policy.state_dict(), f'ppo_checkpoint_ep{START_EPISODE + episode + 1}.pth')\n",
    "    \n",
    "    torch.save(agent.policy.state_dict(), 'ppo_lane_keeping_final_continued.pth')\n",
    "    \n",
    "    print(f\"\\n✓ Continued training completed in {(time.time() - start_time) / 60:.1f} minutes\")\n",
    "    print(f\"✓ Total episodes: {START_EPISODE + EPISODES}\")\n",
    "    print(f\"✓ Best avg distance: {best_distance:.1f}m\")\n",
    "    print(f\"✓ Model saved as: ppo_lane_keeping_best_continued.pth\")\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    continue_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================\n",
    "CONTINUING PPO TRAINING FROM CHECKPOINT\n",
    "================================================================================\n",
    "Checkpoint: ppo_lane_keeping_best.pth\n",
    "Additional episodes: 500\n",
    "Starting from episode: 500\n",
    "\n",
    "carla_rl_training.py:2388: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "  agent.policy.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "✓ Checkpoint loaded successfully!\n",
    "✓ Continuing training from episode 500\n",
    "\n",
    "Resuming training...\n",
    "\n",
    "Ep 510 | R:  1456.8 | D:  456.6m | L: 283 | Avg10: R=4601.4, D=599.5m | Loss: P=-0.001, V=7041.839 | T: 0.6min\n",
    "  ✓ New best! Avg distance: 599.5m\n",
    "Ep 520 | R:  5543.1 | D:  638.1m | L: 800 | Avg10: R=5170.4, D=608.1m | Loss: P=0.012, V=2285.150 | T: 1.3min\n",
    "  ✓ New best! Avg distance: 608.1m\n",
    "Ep 530 | R:  5154.8 | D:  629.6m | L: 800 | Avg10: R=4953.8, D=621.0m | Loss: P=0.017, V=1702.724 | T: 2.1min\n",
    "  ✓ New best! Avg distance: 621.0m\n",
    "Ep 540 | R:  4986.0 | D:  651.1m | L: 800 | Avg10: R=4783.8, D=602.5m | Loss: P=-0.001, V=1583.608 | T: 2.8min\n",
    "Ep 550 | R:  5136.3 | D:  652.7m | L: 800 | Avg10: R=5148.2, D=622.4m | Loss: P=-0.001, V=1798.660 | T: 3.6min\n",
    "  ✓ New best! Avg distance: 622.4m\n",
    "Ep 560 | R:  5162.8 | D:  595.5m | L: 800 | Avg10: R=4976.1, D=627.9m | Loss: P=-0.001, V=1783.139 | T: 4.4min\n",
    "  ✓ New best! Avg distance: 627.9m\n",
    "Ep 570 | R:  5250.6 | D:  630.1m | L: 800 | Avg10: R=5067.0, D=622.1m | Loss: P=0.002, V=1770.697 | T: 5.2min\n",
    "Ep 580 | R:  5347.6 | D:  590.8m | L: 800 | Avg10: R=5223.7, D=605.6m | Loss: P=0.000, V=1984.345 | T: 6.0min\n",
    "Ep 590 | R:  4985.4 | D:  637.7m | L: 800 | Avg10: R=5282.2, D=612.9m | Loss: P=-0.004, V=1823.793 | T: 6.8min\n",
    "Ep 600 | R:  5266.9 | D:  598.4m | L: 800 | Avg10: R=5186.7, D=620.6m | Loss: P=-0.005, V=1817.128 | T: 7.6min\n",
    "Ep 610 | R:  5535.6 | D:  631.2m | L: 800 | Avg10: R=5313.3, D=601.1m | Loss: P=-0.002, V=1963.984 | T: 8.4min\n",
    "Ep 620 | R:  5102.7 | D:  595.5m | L: 800 | Avg10: R=5344.0, D=611.1m | Loss: P=-0.006, V=1786.040 | T: 9.1min\n",
    "Ep 630 | R:  5059.5 | D:  594.9m | L: 800 | Avg10: R=5017.7, D=602.1m | Loss: P=-0.002, V=2378.541 | T: 9.9min\n",
    "Ep 640 | R:  5363.6 | D:  591.1m | L: 800 | Avg10: R=5204.6, D=617.6m | Loss: P=-0.002, V=1868.886 | T: 10.7min\n",
    "Ep 650 | R:  4910.7 | D:  591.2m | L: 800 | Avg10: R=5217.7, D=621.0m | Loss: P=-0.002, V=3035.562 | T: 11.5min\n",
    "Ep 660 | R:  4368.0 | D:  651.4m | L: 800 | Avg10: R=4659.3, D=601.1m | Loss: P=0.000, V=1838.808 | T: 12.1min\n",
    "Ep 670 | R:  4682.1 | D:  657.8m | L: 800 | Avg10: R=4949.1, D=622.1m | Loss: P=-0.002, V=1373.904 | T: 12.9min\n",
    "Ep 680 | R:  5211.6 | D:  619.0m | L: 800 | Avg10: R=5295.1, D=616.7m | Loss: P=-0.003, V=1769.187 | T: 13.6min\n",
    "Ep 690 | R:  3823.3 | D:  633.2m | L: 800 | Avg10: R=2831.5, D=636.7m | Loss: P=-0.004, V=1023.465 | T: 14.4min\n",
    "  ✓ New best! Avg distance: 636.7m\n",
    "Ep 700 | R:  4212.7 | D:  629.9m | L: 800 | Avg10: R=4096.1, D=609.5m | Loss: P=0.000, V=1164.180 | T: 15.2min\n",
    "Ep 710 | R:  5584.6 | D:  658.5m | L: 800 | Avg10: R=4870.5, D=599.5m | Loss: P=-0.001, V=1805.789 | T: 15.9min\n",
    "Ep 720 | R:  5058.9 | D:  650.5m | L: 800 | Avg10: R=5204.3, D=621.1m | Loss: P=0.006, V=1719.087 | T: 16.7min\n",
    "Ep 730 | R:  5250.6 | D:  638.5m | L: 800 | Avg10: R=4927.2, D=609.0m | Loss: P=-0.001, V=1801.079 | T: 17.5min\n",
    "Ep 740 | R:  4960.2 | D:  639.5m | L: 800 | Avg10: R=5011.1, D=623.3m | Loss: P=-0.001, V=1907.849 | T: 18.3min\n",
    "Ep 750 | R:  5153.4 | D:  614.7m | L: 800 | Avg10: R=4719.4, D=606.0m | Loss: P=-0.000, V=1797.341 | T: 18.9min\n",
    "Ep 760 | R:  5389.2 | D:  639.6m | L: 800 | Avg10: R=5315.8, D=634.0m | Loss: P=-0.001, V=2109.483 | T: 19.7min\n",
    "Ep 770 | R:  3969.1 | D:  634.6m | L: 800 | Avg10: R=4288.1, D=624.9m | Loss: P=0.003, V=1248.874 | T: 20.5min\n",
    "Ep 780 | R:  5172.2 | D:  652.2m | L: 800 | Avg10: R=4796.4, D=619.0m | Loss: P=-0.003, V=1563.332 | T: 21.3min\n",
    "Ep 790 | R:  4835.4 | D:  629.8m | L: 800 | Avg10: R=5322.2, D=623.9m | Loss: P=-0.000, V=1576.500 | T: 22.1min\n",
    "Ep 800 | R:  4840.2 | D:  601.6m | L: 800 | Avg10: R=5148.2, D=628.1m | Loss: P=0.004, V=2511.006 | T: 22.8min\n",
    "Ep 810 | R:  5183.0 | D:  613.0m | L: 800 | Avg10: R=4848.7, D=609.3m | Loss: P=-0.002, V=2119.789 | T: 23.6min\n",
    "Ep 820 | R:  5352.9 | D:  591.7m | L: 800 | Avg10: R=4845.7, D=597.9m | Loss: P=-0.002, V=1784.175 | T: 24.3min\n",
    "Ep 830 | R:  5063.5 | D:  629.9m | L: 800 | Avg10: R=5256.9, D=616.0m | Loss: P=-0.001, V=1895.083 | T: 25.1min\n",
    "Ep 840 | R:  5310.1 | D:  598.0m | L: 800 | Avg10: R=5034.1, D=624.5m | Loss: P=-0.002, V=1854.212 | T: 25.8min\n",
    "Ep 850 | R:  5035.0 | D:  629.9m | L: 800 | Avg10: R=5262.4, D=608.9m | Loss: P=-0.005, V=1445.483 | T: 26.6min\n",
    "Ep 860 | R:  5365.6 | D:  621.0m | L: 800 | Avg10: R=5151.3, D=611.6m | Loss: P=-0.001, V=2036.254 | T: 27.4min\n",
    "Ep 870 | R:  5009.2 | D:  658.8m | L: 800 | Avg10: R=5231.5, D=634.0m | Loss: P=-0.003, V=1751.550 | T: 28.1min\n",
    "Ep 880 | R:  5497.9 | D:  636.5m | L: 800 | Avg10: R=5176.1, D=610.1m | Loss: P=-0.002, V=1758.230 | T: 28.8min\n",
    "Ep 890 | R:  5408.5 | D:  614.0m | L: 800 | Avg10: R=5021.2, D=607.2m | Loss: P=-0.000, V=2300.044 | T: 29.6min\n",
    "Ep 900 | R:  5365.1 | D:  598.6m | L: 800 | Avg10: R=4999.7, D=601.1m | Loss: P=-0.000, V=1828.457 | T: 30.3min\n",
    "Ep 910 | R:  5436.8 | D:  597.7m | L: 800 | Avg10: R=5256.2, D=618.1m | Loss: P=-0.001, V=1942.966 | T: 31.1min\n",
    "Ep 920 | R:  4465.2 | D:  600.5m | L: 800 | Avg10: R=5157.5, D=619.1m | Loss: P=0.011, V=2764.228 | T: 31.9min\n",
    "Ep 930 | R:  5137.1 | D:  595.4m | L: 800 | Avg10: R=5089.9, D=617.6m | Loss: P=0.001, V=2434.797 | T: 32.8min\n",
    "Ep 940 | R:  5350.8 | D:  581.1m | L: 800 | Avg10: R=5364.5, D=621.7m | Loss: P=-0.004, V=2088.951 | T: 33.5min\n",
    "Ep 950 | R:  5141.1 | D:  614.5m | L: 800 | Avg10: R=5060.9, D=620.6m | Loss: P=-0.001, V=1833.919 | T: 34.2min\n",
    "Ep 960 | R:  5579.9 | D:  631.8m | L: 800 | Avg10: R=5386.3, D=631.4m | Loss: P=-0.000, V=1842.848 | T: 35.0min\n",
    "Ep 970 | R:  5516.2 | D:  583.9m | L: 800 | Avg10: R=5341.5, D=617.5m | Loss: P=-0.001, V=1948.251 | T: 35.8min\n",
    "Ep 980 | R:  5460.4 | D:  620.4m | L: 800 | Avg10: R=5239.5, D=611.4m | Loss: P=-0.001, V=2147.317 | T: 36.5min\n",
    "Ep 990 | R:  4674.9 | D:  586.7m | L: 800 | Avg10: R=5298.5, D=610.5m | Loss: P=-0.003, V=2513.180 | T: 37.2min\n",
    "Ep 1000 | R:  5081.4 | D:  622.3m | L: 800 | Avg10: R=5364.5, D=620.7m | Loss: P=-0.001, V=2526.472 | T: 38.0min\n",
    "\n",
    "✓ Continued training completed in 38.0 minutes\n",
    "✓ Total episodes: 1000\n",
    "✓ Best avg distance: 636.7m\n",
    "✓ Model saved as: ppo_lane_keeping_best_continued.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PPO Lane Keeping - Evaluation Script\n",
    "Evaluates trained PPO model with video recording\n",
    "\"\"\"\n",
    "\n",
    "import carla\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import imageio.v2 as imageio\n",
    "from datetime import datetime\n",
    "\n",
    "TARGET_SPEED = 7.0\n",
    "\n",
    "# ==================== ACTOR-CRITIC (same as training) ====================\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim=12, action_dim=2):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.actor_mean = nn.Linear(256, action_dim)\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "        self.critic = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        features = self.shared(state)\n",
    "        return features\n",
    "    \n",
    "    def act(self, state, deterministic=False):\n",
    "        features = self.forward(state)\n",
    "        \n",
    "        action_mean = self.actor_mean(features)\n",
    "        \n",
    "        if deterministic:\n",
    "            return action_mean\n",
    "        else:\n",
    "            action_std = torch.exp(self.actor_log_std)\n",
    "            dist = Normal(action_mean, action_std)\n",
    "            action = dist.sample()\n",
    "            return action\n",
    "\n",
    "\n",
    "# ==================== EVALUATION ENVIRONMENT ====================\n",
    "class EvalEnv:\n",
    "    def __init__(self, visualize=True):\n",
    "        self.client = carla.Client('localhost', 2000)\n",
    "        self.client.set_timeout(10.0)\n",
    "        self.world = self.client.load_world('Town04')\n",
    "        \n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = True\n",
    "        settings.fixed_delta_seconds = 0.05\n",
    "        self.world.apply_settings(settings)\n",
    "        \n",
    "        self.blueprint_library = self.world.get_blueprint_library()\n",
    "        self.map = self.world.get_map()\n",
    "        self.vehicle = None\n",
    "        self.sensors = []\n",
    "        self.camera = None\n",
    "        self.visualize = visualize\n",
    "        \n",
    "        self.collision_hist = []\n",
    "        self.spawn_points = self.map.get_spawn_points()\n",
    "        \n",
    "        self.distance_traveled = 0.0\n",
    "        self.last_location = None\n",
    "        self.steps = 0\n",
    "        self.speeds = []\n",
    "        self.lane_offsets = []\n",
    "        \n",
    "        self.recording_frames = []\n",
    "        \n",
    "        self.last_lane_offset = 0.0\n",
    "        self.last_heading_error = 0.0\n",
    "    \n",
    "    def camera_callback(self, image):\n",
    "        try:\n",
    "            if image is None or not hasattr(image, 'raw_data'):\n",
    "                return\n",
    "            if not self.vehicle or not self.vehicle.is_alive:\n",
    "                return\n",
    "            \n",
    "            arr = np.frombuffer(image.raw_data, dtype=np.uint8)\n",
    "            arr = arr.reshape((image.height, image.width, 4))[:, :, :3]\n",
    "            arr = arr[:, :, ::-1]\n",
    "            \n",
    "            overlay = self._create_overlay(arr.copy())\n",
    "            if overlay is not None:\n",
    "                self.recording_frames.append(overlay)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def _create_overlay(self, frame):\n",
    "        try:\n",
    "            if frame is None or frame.size == 0:\n",
    "                return None\n",
    "            \n",
    "            overlay = frame.copy()\n",
    "            cv2.rectangle(overlay, (10, 10), (450, 180), (0, 0, 0), -1)\n",
    "            overlay = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
    "            \n",
    "            if self.vehicle and self.vehicle.is_alive:\n",
    "                velocity = self.vehicle.get_velocity()\n",
    "                speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "                \n",
    "                cv2.putText(overlay, f\"Step: {self.steps}\", (20, 40), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                cv2.putText(overlay, f\"Speed: {speed:.1f} m/s ({speed*3.6:.1f} km/h)\", (20, 70), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                cv2.putText(overlay, f\"Distance: {self.distance_traveled:.1f} m\", (20, 100), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "                \n",
    "                if self.lane_offsets:\n",
    "                    lane_off = self.lane_offsets[-1]\n",
    "                    color = (0, 255, 0) if abs(lane_off) < 0.3 else (0, 165, 255)\n",
    "                    cv2.putText(overlay, f\"Lane: {lane_off:+.3f} m\", (20, 130), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "                \n",
    "                cv2.putText(overlay, \"PPO AGENT\", (20, 160), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)\n",
    "            \n",
    "            return overlay\n",
    "        except:\n",
    "            return frame\n",
    "    \n",
    "    def reset(self, spawn_idx=0):\n",
    "        self._cleanup()\n",
    "        \n",
    "        vehicle_bp = self.blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "        spawn_point = self.spawn_points[spawn_idx % len(self.spawn_points)]\n",
    "        self.vehicle = self.world.spawn_actor(vehicle_bp, spawn_point)\n",
    "        \n",
    "        # Collision sensor\n",
    "        collision_bp = self.blueprint_library.find('sensor.other.collision')\n",
    "        collision_sensor = self.world.spawn_actor(\n",
    "            collision_bp, carla.Transform(), attach_to=self.vehicle\n",
    "        )\n",
    "        collision_sensor.listen(lambda e: self.collision_hist.append(e))\n",
    "        self.sensors.append(collision_sensor)\n",
    "        \n",
    "        # Camera\n",
    "        cam_bp = self.blueprint_library.find('sensor.camera.rgb')\n",
    "        cam_bp.set_attribute('image_size_x', '1280')\n",
    "        cam_bp.set_attribute('image_size_y', '720')\n",
    "        cam_bp.set_attribute('fov', '90')\n",
    "        cam_transform = carla.Transform(\n",
    "            carla.Location(x=-6.0, z=2.8),\n",
    "            carla.Rotation(pitch=-12)\n",
    "        )\n",
    "        self.camera = self.world.spawn_actor(cam_bp, cam_transform, attach_to=self.vehicle)\n",
    "        self.camera.listen(self.camera_callback)\n",
    "        self.sensors.append(self.camera)\n",
    "        \n",
    "        self.collision_hist.clear()\n",
    "        self.distance_traveled = 0.0\n",
    "        self.last_location = self.vehicle.get_location()\n",
    "        self.steps = 0\n",
    "        self.speeds = []\n",
    "        self.lane_offsets = []\n",
    "        self.recording_frames = []\n",
    "        \n",
    "        self.last_lane_offset = 0.0\n",
    "        self.last_heading_error = 0.0\n",
    "        \n",
    "        if self.visualize:\n",
    "            spectator = self.world.get_spectator()\n",
    "            t = self.vehicle.get_transform()\n",
    "            spectator.set_transform(carla.Transform(\n",
    "                t.location + carla.Location(x=-10, z=5),\n",
    "                carla.Rotation(pitch=-20, yaw=t.rotation.yaw)\n",
    "            ))\n",
    "        \n",
    "        for _ in range(10):\n",
    "            self.world.tick()\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Same 12-dim state as training\"\"\"\n",
    "        transform = self.vehicle.get_transform()\n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        self.speeds.append(speed)\n",
    "        \n",
    "        waypoint = self.map.get_waypoint(\n",
    "            transform.location, project_to_road=True, lane_type=carla.LaneType.Driving\n",
    "        )\n",
    "        \n",
    "        if not waypoint:\n",
    "            return np.zeros(12, dtype=np.float32)\n",
    "        \n",
    "        lane_center = waypoint.transform.location\n",
    "        offset_vec = transform.location - lane_center\n",
    "        right_vec = transform.get_right_vector()\n",
    "        lane_offset = offset_vec.x * right_vec.x + offset_vec.y * right_vec.y\n",
    "        \n",
    "        self.lane_offsets.append(abs(lane_offset))\n",
    "        \n",
    "        lane_yaw = waypoint.transform.rotation.yaw\n",
    "        vehicle_yaw = transform.rotation.yaw\n",
    "        heading_error = (vehicle_yaw - lane_yaw + 180) % 360 - 180\n",
    "        heading_error_rad = np.radians(heading_error)\n",
    "        \n",
    "        next_wps = waypoint.next(10.0)\n",
    "        if next_wps:\n",
    "            next_wp = next_wps[0]\n",
    "            wp_vec = next_wp.transform.location - transform.location\n",
    "            forward_vec = transform.get_forward_vector()\n",
    "            angle_to_wp = np.arctan2(\n",
    "                wp_vec.y * forward_vec.x - wp_vec.x * forward_vec.y,\n",
    "                wp_vec.x * forward_vec.x + wp_vec.y * forward_vec.y\n",
    "            )\n",
    "        else:\n",
    "            angle_to_wp = 0.0\n",
    "        \n",
    "        curvature = 0.0\n",
    "        if next_wps:\n",
    "            curvature_yaw = next_wps[0].transform.rotation.yaw - lane_yaw\n",
    "            curvature = np.sin(np.radians(curvature_yaw))\n",
    "        \n",
    "        speed_norm = speed / 15.0\n",
    "        vel_x = velocity.x / 15.0\n",
    "        vel_y = velocity.y / 15.0\n",
    "        is_junction = 1.0 if waypoint.is_junction else 0.0\n",
    "        \n",
    "        state = np.array([\n",
    "            lane_offset / 3.0,\n",
    "            np.sin(heading_error_rad),\n",
    "            np.cos(heading_error_rad),\n",
    "            np.sin(angle_to_wp),\n",
    "            np.cos(angle_to_wp),\n",
    "            curvature,\n",
    "            speed_norm,\n",
    "            vel_x,\n",
    "            vel_y,\n",
    "            is_junction,\n",
    "            lane_offset / 3.0 - self.last_lane_offset / 3.0,\n",
    "            heading_error_rad - self.last_heading_error,\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        self.last_lane_offset = lane_offset\n",
    "        self.last_heading_error = heading_error_rad\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        steering = float(np.clip(action[0], -0.4, 0.4))\n",
    "        \n",
    "        velocity = self.vehicle.get_velocity()\n",
    "        speed = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2)\n",
    "        \n",
    "        speed_error = TARGET_SPEED - speed\n",
    "        base_throttle = 0.5 + 0.1 * speed_error\n",
    "        throttle_delta = float(np.clip(action[1], -0.3, 0.3))\n",
    "        throttle = float(np.clip(base_throttle + throttle_delta, 0.0, 0.8))\n",
    "        \n",
    "        control = carla.VehicleControl(\n",
    "            throttle=throttle,\n",
    "            steer=steering,\n",
    "            brake=0.0\n",
    "        )\n",
    "        \n",
    "        self.vehicle.apply_control(control)\n",
    "        \n",
    "        if self.visualize:\n",
    "            try:\n",
    "                spectator = self.world.get_spectator()\n",
    "                t = self.vehicle.get_transform()\n",
    "                spec_location = t.location - t.get_forward_vector() * 10 + carla.Location(z=5)\n",
    "                spectator.set_transform(carla.Transform(\n",
    "                    spec_location,\n",
    "                    carla.Rotation(pitch=-20, yaw=t.rotation.yaw)\n",
    "                ))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        self.world.tick()\n",
    "        self.steps += 1\n",
    "        \n",
    "        current_location = self.vehicle.get_location()\n",
    "        if self.last_location:\n",
    "            self.distance_traveled += current_location.distance(self.last_location)\n",
    "        self.last_location = current_location\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        done = self._is_done()\n",
    "        \n",
    "        return next_state, done\n",
    "    \n",
    "    def _is_done(self):\n",
    "        if len(self.collision_hist) > 0:\n",
    "            print(f\"\\n  ⚠️  COLLISION at step {self.steps}\")\n",
    "            return True\n",
    "        \n",
    "        transform = self.vehicle.get_transform()\n",
    "        waypoint = self.map.get_waypoint(transform.location, project_to_road=True)\n",
    "        \n",
    "        if waypoint is None:\n",
    "            print(f\"\\n  ⚠️  OFF ROAD\")\n",
    "            return True\n",
    "        \n",
    "        if transform.location.distance(waypoint.transform.location) > 3.5:\n",
    "            print(f\"\\n  ⚠️  TOO FAR FROM LANE\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def save_video(self, episode_num, output_dir=\"ppo_eval_videos\"):\n",
    "        if not self.recording_frames:\n",
    "            return None\n",
    "        \n",
    "        valid_frames = [f for f in self.recording_frames if f is not None and f.size > 0]\n",
    "        if not valid_frames:\n",
    "            return None\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        video_path = os.path.join(output_dir, f\"ppo_ep{episode_num}_{timestamp}.mp4\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"  Saving video ({len(valid_frames)} frames)...\")\n",
    "            with imageio.get_writer(video_path, fps=20, codec='libx264', quality=8, pixelformat='yuv420p') as writer:\n",
    "                for frame in valid_frames:\n",
    "                    try:\n",
    "                        writer.append_data(frame)\n",
    "                    except:\n",
    "                        continue\n",
    "            print(f\"  ✓ Video: {video_path}\")\n",
    "            return video_path\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Video failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        if self.camera and self.camera.is_alive:\n",
    "            try:\n",
    "                self.camera.stop()\n",
    "                time.sleep(0.1)\n",
    "            except:\n",
    "                pass\n",
    "        if self.vehicle and self.vehicle.is_alive:\n",
    "            try:\n",
    "                self.vehicle.destroy()\n",
    "            except:\n",
    "                pass\n",
    "        for sensor in self.sensors:\n",
    "            try:\n",
    "                if sensor.is_alive:\n",
    "                    sensor.destroy()\n",
    "            except:\n",
    "                pass\n",
    "        self.sensors.clear()\n",
    "        self.vehicle = None\n",
    "        self.camera = None\n",
    "    \n",
    "    def close(self):\n",
    "        self._cleanup()\n",
    "        settings = self.world.get_settings()\n",
    "        settings.synchronous_mode = False\n",
    "        self.world.apply_settings(settings)\n",
    "\n",
    "\n",
    "# ==================== EVALUATION ====================\n",
    "def evaluate(model_path, num_episodes=5, max_steps=2000, deterministic=True):\n",
    "    print(\"=\"*80)\n",
    "    print(\"PPO LANE KEEPING - EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model: {model_path}\")\n",
    "    print(f\"Episodes: {num_episodes}\")\n",
    "    print(f\"Deterministic: {deterministic}\\n\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy = ActorCritic(state_dim=12, action_dim=2).to(device)\n",
    "    \n",
    "    try:\n",
    "        policy.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        policy.eval()\n",
    "        print(f\"✓ Model loaded\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load: {e}\")\n",
    "        return\n",
    "    \n",
    "    env = EvalEnv(visualize=True)\n",
    "    \n",
    "    all_distances = []\n",
    "    all_speeds = []\n",
    "    all_lane_offsets = []\n",
    "    all_collisions = 0\n",
    "    video_paths = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EPISODE {ep+1}/{num_episodes}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        state = env.reset(spawn_idx=ep * 15)\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action = policy.act(state_tensor, deterministic=deterministic)\n",
    "            \n",
    "            next_state, done = env.step(action.cpu().numpy()[0])\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if steps % 200 == 0:\n",
    "                avg_speed = np.mean(env.speeds[-100:]) if len(env.speeds) >= 100 else np.mean(env.speeds)\n",
    "                avg_lane = np.mean(env.lane_offsets[-100:]) if len(env.lane_offsets) >= 100 else np.mean(env.lane_offsets)\n",
    "                print(f\"  Step {steps:4d}: Speed={avg_speed:.1f} m/s | Dist={env.distance_traveled:.1f}m | Lane={avg_lane:.3f}m\")\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"EPISODE {ep+1} SUMMARY\")\n",
    "        print(f\"{'─'*80}\\n\")\n",
    "        \n",
    "        outcome = \"✗ FAILED\" if len(env.collision_hist) > 0 else \"✓ SUCCESS\"\n",
    "        print(f\"Outcome:     {outcome}\")\n",
    "        print(f\"Distance:    {env.distance_traveled:.1f} m\")\n",
    "        print(f\"Steps:       {steps}/{max_steps}\")\n",
    "        print(f\"Avg Speed:   {np.mean(env.speeds):.1f} m/s ({np.mean(env.speeds)*3.6:.1f} km/h)\")\n",
    "        print(f\"Avg Lane:    {np.mean(env.lane_offsets):.3f} m\")\n",
    "        print(f\"Max Lane:    {max(env.lane_offsets) if env.lane_offsets else 0:.3f} m\\n\")\n",
    "        \n",
    "        all_distances.append(env.distance_traveled)\n",
    "        all_speeds.append(np.mean(env.speeds))\n",
    "        all_lane_offsets.append(np.mean(env.lane_offsets))\n",
    "        if len(env.collision_hist) > 0:\n",
    "            all_collisions += 1\n",
    "        \n",
    "        video_path = env.save_video(ep + 1)\n",
    "        if video_path:\n",
    "            video_paths.append(video_path)\n",
    "        \n",
    "        print()\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Overall results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OVERALL RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    success_rate = ((num_episodes - all_collisions) / num_episodes) * 100\n",
    "    print(f\"Success Rate:       {success_rate:.1f}%\")\n",
    "    print(f\"Avg Distance:       {np.mean(all_distances):.1f} m\")\n",
    "    print(f\"Avg Speed:          {np.mean(all_speeds):.1f} m/s ({np.mean(all_speeds)*3.6:.1f} km/h)\")\n",
    "    print(f\"Avg Lane Offset:    {np.mean(all_lane_offsets):.3f} m\\n\")\n",
    "    \n",
    "    # Scoring\n",
    "    speed_score = min(100, (np.mean(all_speeds) / 7.0) * 100)\n",
    "    safety_score = success_rate\n",
    "    distance_score = min(100, (np.mean(all_distances) / 1000.0) * 100)\n",
    "    lane_score = max(0, 100 - (np.mean(all_lane_offsets) / 0.5) * 100)\n",
    "    \n",
    "    overall_score = (speed_score + safety_score + distance_score + lane_score) / 4\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"SCORING\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"Speed:          {speed_score:.1f}%\")\n",
    "    print(f\"Safety:         {safety_score:.1f}%\")\n",
    "    print(f\"Distance:       {distance_score:.1f}%\")\n",
    "    print(f\"Lane Keeping:   {lane_score:.1f}%\")\n",
    "    print(f\"OVERALL:        {overall_score:.1f}%\\n\")\n",
    "    \n",
    "    if overall_score >= 85:\n",
    "        grade = \"A - Excellent\"\n",
    "    elif overall_score >= 75:\n",
    "        grade = \"B - Good\"\n",
    "    elif overall_score >= 65:\n",
    "        grade = \"C - Fair\"\n",
    "    else:\n",
    "        grade = \"D - Needs Improvement\"\n",
    "    \n",
    "    print(f\"GRADE: {grade}\\n\")\n",
    "    \n",
    "    print(\"VIDEOS:\")\n",
    "    for vp in video_paths:\n",
    "        print(f\"  • {vp}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', type=str, default='ppo_lane_keeping_best.pth')\n",
    "    parser.add_argument('--episodes', type=int, default=5)\n",
    "    parser.add_argument('--max_steps', type=int, default=2000)\n",
    "    parser.add_argument('--stochastic', action='store_true', help='Use stochastic policy')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(\"\\n⚠️  Make sure CARLA is running!\\n\")\n",
    "    input(\"Press Enter to start...\")\n",
    "    \n",
    "    evaluate(args.model, args.episodes, args.max_steps, deterministic=not args.stochastic)\n",
    "\n",
    "    #VIDEOS: ppo_eval_videos/ppo_ep1_20260103_194607.mp4 inside PPO_best_git folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "================================================================================\n",
    "\n",
    "EPISODE 1/1\n",
    "\n",
    "================================================================================\n",
    "\n",
    "  Step  200: Speed=5.1 m/s | Dist=467.1m | Lane=0.145m\n",
    "\n",
    "  Step  400: Speed=5.1 m/s | Dist=518.0m | Lane=0.172m\n",
    "\n",
    "  Step  600: Speed=5.1 m/s | Dist=568.9m | Lane=0.143m\n",
    "\n",
    "  Step  800: Speed=5.1 m/s | Dist=619.8m | Lane=0.101m\n",
    "\n",
    "  Step 1000: Speed=5.1 m/s | Dist=670.7m | Lane=0.042m\n",
    "\n",
    "  Step 1200: Speed=5.1 m/s | Dist=721.5m | Lane=0.076m\n",
    "\n",
    "  Step 1400: Speed=5.0 m/s | Dist=771.9m | Lane=0.238m\n",
    "\n",
    "  Step 1600: Speed=5.1 m/s | Dist=822.3m | Lane=0.236m\n",
    "\n",
    "  Step 1800: Speed=5.0 m/s | Dist=872.7m | Lane=0.317m\n",
    "\n",
    "  Step 2000: Speed=5.0 m/s | Dist=922.9m | Lane=0.442m\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "EPISODE 1 SUMMARY\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "Outcome:     ✓ SUCCESS\n",
    "\n",
    "Distance:    922.9 m\n",
    "\n",
    "Steps:       2000/2000\n",
    "\n",
    "Avg Speed:   5.0 m/s (17.8 km/h)\n",
    "\n",
    "Avg Lane:    0.175 m\n",
    "\n",
    "Max Lane:    1.667 m\n",
    "\n",
    "  Saving video (1462 frames)...\n",
    "\n",
    "  ✓ Video: ppo_eval_videos/ppo_ep1_20260103_194607.mp4\n",
    "\n",
    "================================================================================\n",
    "\n",
    "OVERALL RESULTS\n",
    "\n",
    "================================================================================\n",
    "\n",
    "Success Rate:       100.0%\n",
    "\n",
    "Avg Distance:       922.9 m\n",
    "\n",
    "Avg Speed:          5.0 m/s (17.8 km/h)\n",
    "\n",
    "Avg Lane Offset:    0.175 m\n",
    "\n",
    "================================================================================\n",
    "\n",
    "SCORING\n",
    "\n",
    "================================================================================\n",
    "\n",
    "Speed:          70.8%\n",
    "\n",
    "Safety:         100.0%\n",
    "\n",
    "Distance:       92.3%\n",
    "\n",
    "Lane Keeping:   65.0%\n",
    "\n",
    "OVERALL:        82.0%\n",
    "\n",
    "GRADE: B - Good\n",
    "\n",
    "VIDEOS:\n",
    "\n",
    "  • ppo_eval_videos/ppo_ep1_20260103_194607.mp4\n",
    "\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " python3 carla_evaluation.py  --model /home/robotuser/carla_0.9.12/PythonAPI/examples/ppo_lane_keeping_final_continued.pth --episodes 1 --max_steps 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6:14:17 (carla_env) robotuser@kuorobot02 examples → python3 carla_evaluation.py  --model /home/robotuser/carla_0.9.12/PythonAPI/examples/ppo_lane_keeping_final_continued.pth --episodes 2 --max_steps 2000\n",
    "\n",
    "⚠️  Make sure CARLA is running!\n",
    "\n",
    "Press Enter to start...\n",
    "================================================================================\n",
    "PPO LANE KEEPING - EVALUATION\n",
    "================================================================================\n",
    "Model: /home/robotuser/carla_0.9.12/PythonAPI/examples/ppo_lane_keeping_final_continued.pth\n",
    "Episodes: 2\n",
    "Deterministic: True\n",
    "\n",
    "carla_evaluation.py:3226: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "  policy.load_state_dict(torch.load(model_path, map_location=device))\n",
    "✓ Model loaded\n",
    "\n",
    "\n",
    "================================================================================\n",
    "EPISODE 1/2\n",
    "================================================================================\n",
    "\n",
    "  Step  200: Speed=5.1 m/s | Dist=467.0m | Lane=0.049m\n",
    "  Step  400: Speed=5.1 m/s | Dist=517.6m | Lane=0.114m\n",
    "  Step  600: Speed=5.1 m/s | Dist=568.2m | Lane=0.161m\n",
    "  Step  800: Speed=5.1 m/s | Dist=618.8m | Lane=0.169m\n",
    "  Step 1000: Speed=5.1 m/s | Dist=669.4m | Lane=0.136m\n",
    "  Step 1200: Speed=5.1 m/s | Dist=720.0m | Lane=0.222m\n",
    "  Step 1400: Speed=5.1 m/s | Dist=770.6m | Lane=0.171m\n",
    "  Step 1600: Speed=5.1 m/s | Dist=821.2m | Lane=0.140m\n",
    "  Step 1800: Speed=5.2 m/s | Dist=872.3m | Lane=0.425m\n",
    "  Step 2000: Speed=5.1 m/s | Dist=923.1m | Lane=0.112m\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "EPISODE 1 SUMMARY\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "Outcome:     ✓ SUCCESS\n",
    "Distance:    923.1 m\n",
    "Steps:       2000/2000\n",
    "Avg Speed:   5.0 m/s (17.8 km/h)\n",
    "Avg Lane:    0.146 m\n",
    "Max Lane:    1.351 m\n",
    "\n",
    "  Saving video (1323 frames)...\n",
    "  ✓ Video: ppo_eval_videos/ppo_ep1_20260108_181515.mp4\n",
    "\n",
    "\n",
    "================================================================================\n",
    "EPISODE 2/2\n",
    "================================================================================\n",
    "\n",
    "  Step  200: Speed=5.1 m/s | Dist=467.8m | Lane=0.096m\n",
    "  Step  400: Speed=5.1 m/s | Dist=518.4m | Lane=0.103m\n",
    "  Step  600: Speed=5.1 m/s | Dist=569.0m | Lane=0.095m\n",
    "  Step  800: Speed=5.1 m/s | Dist=619.6m | Lane=0.061m\n",
    "  Step 1000: Speed=5.0 m/s | Dist=670.0m | Lane=0.047m\n",
    "  Step 1200: Speed=5.0 m/s | Dist=720.1m | Lane=0.012m\n",
    "  Step 1400: Speed=4.9 m/s | Dist=769.1m | Lane=0.020m\n",
    "  Step 1600: Speed=4.9 m/s | Dist=817.9m | Lane=0.018m\n",
    "  Step 1800: Speed=5.0 m/s | Dist=867.9m | Lane=0.012m\n",
    "  Step 2000: Speed=5.1 m/s | Dist=918.3m | Lane=0.237m\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "EPISODE 2 SUMMARY\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "Outcome:     ✓ SUCCESS\n",
    "Distance:    918.3 m\n",
    "Steps:       2000/2000\n",
    "Avg Speed:   4.9 m/s (17.6 km/h)\n",
    "Avg Lane:    0.067 m\n",
    "Max Lane:    0.242 m\n",
    "\n",
    "  Saving video (1260 frames)...\n",
    "  ✓ Video: ppo_eval_videos/ppo_ep2_20260108_181554.mp4\n",
    "\n",
    "\n",
    "================================================================================\n",
    "OVERALL RESULTS\n",
    "================================================================================\n",
    "\n",
    "Success Rate:       100.0%\n",
    "Avg Distance:       920.7 m\n",
    "Avg Speed:          4.9 m/s (17.7 km/h)\n",
    "Avg Lane Offset:    0.106 m\n",
    "\n",
    "================================================================================\n",
    "SCORING\n",
    "================================================================================\n",
    "\n",
    "Speed:          70.3%\n",
    "Safety:         100.0%\n",
    "Distance:       92.1%\n",
    "Lane Keeping:   78.8%\n",
    "OVERALL:        85.3%\n",
    "\n",
    "GRADE: A - Excellent\n",
    "\n",
    "VIDEOS:\n",
    "  • ppo_eval_videos/ppo_ep1_20260108_181515.mp4\n",
    "  • ppo_eval_videos/ppo_ep2_20260108_181554.mp4\n",
    "\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================\n",
    "PPO LANE KEEPING - EVALUATION\n",
    "================================================================================\n",
    "Model: /home/robotuser/carla_0.9.12/PythonAPI/examples/ppo_lane_keeping_final_continued.pth\n",
    "Episodes: 1\n",
    "Deterministic: True\n",
    "\n",
    "carla_evaluation.py:3226: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "  policy.load_state_dict(torch.load(model_path, map_location=device))\n",
    "✓ Model loaded\n",
    "\n",
    "\n",
    "================================================================================\n",
    "EPISODE 1/1\n",
    "================================================================================\n",
    "\n",
    "  Step  200: Speed=5.1 m/s | Dist=467.0m | Lane=0.049m\n",
    "  Step  400: Speed=5.1 m/s | Dist=517.6m | Lane=0.114m\n",
    "  Step  600: Speed=5.1 m/s | Dist=568.2m | Lane=0.161m\n",
    "  Step  800: Speed=5.1 m/s | Dist=618.8m | Lane=0.169m\n",
    "  Step 1000: Speed=5.1 m/s | Dist=669.4m | Lane=0.136m\n",
    "  Step 1200: Speed=5.1 m/s | Dist=720.0m | Lane=0.222m\n",
    "  Step 1400: Speed=5.1 m/s | Dist=770.6m | Lane=0.171m\n",
    "  Step 1600: Speed=5.1 m/s | Dist=821.2m | Lane=0.140m\n",
    "  Step 1800: Speed=5.2 m/s | Dist=872.3m | Lane=0.425m\n",
    "  Step 2000: Speed=5.1 m/s | Dist=923.1m | Lane=0.112m\n",
    "  Step 2200: Speed=5.1 m/s | Dist=974.1m | Lane=0.237m\n",
    "  Step 2400: Speed=5.1 m/s | Dist=1024.7m | Lane=0.006m\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "EPISODE 1 SUMMARY\n",
    "────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "Outcome:     ✓ SUCCESS\n",
    "Distance:    1050.1 m\n",
    "Steps:       2500/2500\n",
    "Avg Speed:   5.0 m/s (17.9 km/h)\n",
    "Avg Lane:    0.156 m\n",
    "Max Lane:    1.351 m\n",
    "\n",
    "  Saving video (1537 frames)...\n",
    "  ✓ Video: ppo_eval_videos/ppo_ep1_20260108_183655.mp4\n",
    "\n",
    "\n",
    "================================================================================\n",
    "OVERALL RESULTS\n",
    "================================================================================\n",
    "\n",
    "Success Rate:       100.0%\n",
    "Avg Distance:       1050.1 m\n",
    "Avg Speed:          5.0 m/s (17.9 km/h)\n",
    "Avg Lane Offset:    0.156 m\n",
    "\n",
    "================================================================================\n",
    "SCORING\n",
    "================================================================================\n",
    "\n",
    "Speed:          71.1%\n",
    "Safety:         100.0%\n",
    "Distance:       100.0%\n",
    "Lane Keeping:   68.8%\n",
    "OVERALL:        85.0%\n",
    "\n",
    "GRADE: B - Good\n",
    "\n",
    "VIDEOS:\n",
    "  • ppo_eval_videos/ppo_ep1_20260108_183655.mp4\n",
    "\n",
    "================================================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
